<section id="exploring-kernel-functions-in-support-vector-machines-svm" class="cell markdown" id="uwclTNX8eaua">
<h1><strong>Exploring Kernel Functions in Support Vector Machines (SVM)</strong></h1>
<p>A Practical and Visual Guide to Understanding the Role of Kernels and Hyperparameter Tuning in SVMs</p>
<hr />
<h2 id="introduction"><strong>INTRODUCTION</strong></h2>
<p>Support Vector Machines (SVMs) are powerful tools in the machine learning arsenal, designed to tackle both classification and regression tasks with precision and accuracy. What sets SVM apart is its ability to find the optimal boundary, or hyperplane, that separates different classes of data points. But what happens when the data isn't linearly separable? Enter kernels – the secret ingredient that transforms SVM into a versatile powerhouse capable of handling even the most complex datasets.</p>
<p>The goal of a support vector machine is to find the optimal separating hyperplane which maximizes the margin of the training data <a href="https://www.svm-tutorial.com/2014/11/svm-understanding-math-part-1/">SVM Tutorial, 2014</a>.</p>
<p>Kernels are mathematical functions that enable SVM to map data into higher-dimensional spaces, where non-linear patterns become linearly separable. This process, known as the kernel trick, allows SVM to achieve exceptional performance without the computational overhead of explicitly transforming the data.</p>
<p>In this guide, we’ll explore how different kernel functions work, how to tune their hyperparameters effectively, and how they shape decision boundaries. Using a mix of theoretical explanations, practical examples, and visualizations, you’ll gain a deep understanding of how to harness the full potential of SVMs for your own projects.</p>
<hr />
<h3 id="why-kernels-matter"><strong>Why Kernels Matter</strong></h3>
<p>Kernels are the key to unlocking SVM’s ability to handle non-linear relationships in data. By using the right kernel, you can transform an SVM from a simple classifier to a sophisticated tool capable of solving complex real-world problems.</p>
<p>From spam detection to medical diagnostics and image recognition, SVMs equipped with kernels have proven their worth across industries. Understanding how kernels work and learning to fine-tune them is essential for building accurate and efficient machine learning models.</p>
<p>By the end of this guide, you’ll not only know how to implement SVMs with kernels but also how to visualize their behavior, tune their parameters, and select the right kernel for the job.</p>
<h3 id="what-youll-learn"><strong>What You'll Learn</strong></h3>
<ul>
<li><p>How Support Vector Machines classify data with the help of kernel functions.</p></li>
<li><p>The differences between linear, polynomial, and radial basis function (RBF) kernels, and when to use each one.</p></li>
<li><p>How hyperparameter tuning impacts decision boundaries and overall performance.</p></li>
<li><p>How to visualize SVM decision boundaries using interactive Python plots.</p></li>
</ul>
<p>This tutorial bridges theory and practice, showing how to apply SVMs with different kernels in real-world scenarios.</p>
<hr />
<h3 id="why-it-matters"><strong>Why It Matters</strong></h3>
<p>SVMs are widely used in tasks that demand high accuracy and precision, such as:</p>
<ul>
<li><p>Spam detection: Sorting unwanted emails effectively.</p></li>
<li><p>Medical diagnostics: Classifying diseases based on patient data.</p></li>
<li><p>Image recognition: Identifying objects, faces, or patterns in images.</p></li>
</ul>
<p>By mastering kernels, you’ll gain the ability to tailor SVMs to different types of data, unlocking their full potential. This skill is not just theoretical – it’s a practical, job-ready capability for tackling real-world challenges.</p>
<p>Let’s dive in and explore how kernels transform SVMs into versatile, high-performance tools for machine learning.</p>
</section>
<section id="objectives" class="cell markdown" id="yqTpr_ISwL_1">
<h2><strong>OBJECTIVES</strong></h2>
<p>In this section, we outline the key goals of the tutorial.</p>
<ol>
<li>Explain the theory behind SVM and kernel functions.</li>
<li>Show how hyperparameter tuning impacts decision boundaries and performance.</li>
<li>Compare kernel effectiveness using visualizations and performance metrics.</li>
</ol>
</section>
<section id="theoretical-background" class="cell markdown" id="MDnDY7jIemzk">
<h2><strong>THEORETICAL BACKGROUND</strong></h2>
<h4 id="what-are-svms">What are SVMs?</h4>
<p>A supervised learning algorithm that finds the optimal hyperplane to separate data points into classes. Support Vector Machines work by finding a hyperplane that separates data points into different classes. When data is not linearly separable, <strong>kernel functions</strong> are used to map the data into a higher-dimensional space where a linear separator can exist.</p>
<h3 id="common-kernels">Common Kernels</h3>
<ul>
<li><p><strong>Linear Kernel</strong>: Computes a simple dot product between data points in the input space.</p></li>
<li><p>( K(x_i, x_j) = x_i \cdot x_j )</p></li>
</ul>
<p>Great for datasets where the data is already linearly separable. It’s often used for text classification problems, like spam detection.</p>
<ul>
<li><strong>Polynomial Kernel</strong>: Captures polynomial relationships between data points.</li>
<li>( K(x_i, x_j) = (x_i \cdot x_j + c)^d ) Captures complex, curved relationships in data. It’s useful for moderately intricate tasks like certain image classifications.</li>
<li><strong>Radial Basis Function (RBF) Kernel</strong>: Maps data points into an infinite-dimensional space.</li>
<li>( K(x_i, x_j) = \exp(-\gamma |x_i - x_j|^2) )</li>
</ul>
<p>The most flexible option, perfect for data with non-linear patterns, like recognizing objects in images or patterns in biological data.</p>
</section>
<section id="practical-implementation" class="cell markdown" id="_CcoVrmK0vkv">
<h2><strong>PRACTICAL IMPLEMENTATION</strong></h2>
<h3 id="dataset-preparation">Dataset Preparation</h3>
<p>We use two datasets for this tutorial:</p>
<h3 id="iris-dataset">Iris Dataset</h3>
<p>The Iris dataset is a well-known dataset comprising three flower species(Setosa, Versicolor, Virginica) with four features each. It includes both linearly separable and non-linear class distributions. There are <strong>150 samples</strong> in total, with each sample described by <strong>four features</strong>:</p>
<ul>
<li>Sepal Length - Length of the sepal (cm)</li>
<li>Sepal Width - Width of the sepal (cm)</li>
<li>Petal Length - Length of the petal(cm)</li>
<li>Petal Width - Width of the petal (cm)</li>
</ul>
<p>This dataset is perfect for demonstrating SVM kernels because it includes both linearly separable and non-linear class distributions.</p>
<hr />
<h3 id="synthetic-dataset">Synthetic Dataset</h3>
<p>To make things even clearer, we’ll use synthetic data generated with two distinct clusters. This helps us visualize how SVM kernels draw boundaries between classes.</p>
</section>
<div class="cell markdown" id="6p28iJHT5fyZ">
<p>### <strong>Step 1: Load and Prepare Data</strong> The below code block loads the Iris dataset and prepares it for training and testing. The dataset is split into training and testing sets, and the features are standardized using StandardScaler to ensure uniformity and improve SVM performance.</p>
</div>
<div class="cell code" data-execution_count="1" data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}" id="xNysh9LifKfj" data-outputId="1149d51c-dfb6-4cc7-faa7-6f02814cad52">
<div class="sourceCode" id="cb1"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true"></a><span class="im">from</span> sklearn.datasets <span class="im">import</span> load_iris</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true"></a><span class="im">from</span> sklearn.preprocessing <span class="im">import</span> StandardScaler</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true"></a></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true"></a><span class="co"># Step 1: Load the Iris dataset</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true"></a><span class="co"># The Iris dataset contains 150 samples of 3 classes (Setosa, Versicolour, Virginica) with 4 features each.</span></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true"></a>data <span class="op">=</span> load_iris()</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true"></a>X, y <span class="op">=</span> data.data[:, :<span class="dv">2</span>], data.target  <span class="co"># Use only the first two features for visualization</span></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true"></a></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true"></a><span class="co"># Step 2: Split the dataset into training and testing sets</span></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true"></a><span class="co"># 70% of the data is used for training, and 30% is reserved for testing</span></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true"></a>X_train, X_test, y_train, y_test <span class="op">=</span> train_test_split(X, y, test_size<span class="op">=</span><span class="fl">0.3</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true"></a></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true"></a><span class="co"># Step 3: Normalize the feature data</span></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true"></a><span class="co"># StandardScaler standardizes features by removing the mean and scaling to unit variance</span></span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true"></a>scaler <span class="op">=</span> StandardScaler()</span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true"></a>X_train <span class="op">=</span> scaler.fit_transform(X_train)  <span class="co"># Fit and transform training data</span></span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true"></a>X_test <span class="op">=</span> scaler.transform(X_test)        <span class="co"># Transform testing data using the same scaler</span></span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true"></a><span class="bu">print</span>(<span class="st">&quot;Data preprocessed successfully!&quot;</span>)</span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true"></a></span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true"></a></span></code></pre></div>
<div class="output stream stdout">
<pre><code>Data preprocessed successfully!
</code></pre>
</div>
</div>
<section id="step-2-train-models-with-different-kernels" class="cell markdown" id="qM6_cyLPfS5j">
<h3><strong>Step 2: Train Models with Different Kernels</strong></h3>
<p>We will train SVM models with three kernels: Linear, Polynomial, and RBF. Each model will be evaluated using accuracy and confusion matrices.</p>
</section>
<div class="cell code" data-execution_count="2" data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}" id="cFeBLBv2fUzt" data-outputId="65542701-2292-4fa5-d9d4-a003c82d2bed">
<div class="sourceCode" id="cb3"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true"></a><span class="im">from</span> sklearn.svm <span class="im">import</span> SVC</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true"></a></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true"></a><span class="co"># Training Function</span></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true"></a><span class="kw">def</span> train_svm(kernel, params<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true"></a>    <span class="co">&quot;&quot;&quot;</span></span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true"></a><span class="co">    Trains an SVM model using the specified kernel and parameters.</span></span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true"></a></span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true"></a><span class="co">    Parameters:</span></span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true"></a><span class="co">    - kernel (str): Kernel type (&#39;linear&#39;, &#39;poly&#39;, &#39;rbf&#39;).</span></span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true"></a><span class="co">    - params (dict): Additional kernel parameters.</span></span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true"></a></span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true"></a><span class="co">    Returns:</span></span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true"></a><span class="co">    - model (SVC): Trained SVM model.</span></span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true"></a><span class="co">    &quot;&quot;&quot;</span></span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true"></a>    params <span class="op">=</span> params <span class="cf">if</span> params <span class="cf">else</span> {}</span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true"></a>    model <span class="op">=</span> SVC(kernel<span class="op">=</span>kernel, random_state<span class="op">=</span><span class="dv">42</span>, <span class="op">**</span>params)</span>
<span id="cb3-17"><a href="#cb3-17" aria-hidden="true"></a>    model.fit(X_train, y_train)</span>
<span id="cb3-18"><a href="#cb3-18" aria-hidden="true"></a>    <span class="cf">return</span> model</span>
<span id="cb3-19"><a href="#cb3-19" aria-hidden="true"></a></span>
<span id="cb3-20"><a href="#cb3-20" aria-hidden="true"></a><span class="co"># Train models with different kernels</span></span>
<span id="cb3-21"><a href="#cb3-21" aria-hidden="true"></a>kernels <span class="op">=</span> [<span class="st">&#39;linear&#39;</span>, <span class="st">&#39;poly&#39;</span>, <span class="st">&#39;rbf&#39;</span>]</span>
<span id="cb3-22"><a href="#cb3-22" aria-hidden="true"></a>params_list <span class="op">=</span> [<span class="va">None</span>, {<span class="st">&#39;degree&#39;</span>: <span class="dv">3</span>}, {<span class="st">&#39;gamma&#39;</span>: <span class="fl">0.5</span>}]  <span class="co"># Parameters for Polynomial and RBF kernels</span></span>
<span id="cb3-23"><a href="#cb3-23" aria-hidden="true"></a>trained_models <span class="op">=</span> {}</span>
<span id="cb3-24"><a href="#cb3-24" aria-hidden="true"></a></span>
<span id="cb3-25"><a href="#cb3-25" aria-hidden="true"></a><span class="cf">for</span> kernel, params <span class="kw">in</span> <span class="bu">zip</span>(kernels, params_list):</span>
<span id="cb3-26"><a href="#cb3-26" aria-hidden="true"></a>    <span class="bu">print</span>(<span class="ss">f&quot;</span><span class="ch">\n</span><span class="ss">Training SVM with </span><span class="sc">{</span>kernel<span class="sc">.</span>capitalize()<span class="sc">}</span><span class="ss"> Kernel...&quot;</span>)</span>
<span id="cb3-27"><a href="#cb3-27" aria-hidden="true"></a>    trained_models[kernel] <span class="op">=</span> train_svm(kernel, params)  <span class="co"># Store the trained model</span></span>
<span id="cb3-28"><a href="#cb3-28" aria-hidden="true"></a></span></code></pre></div>
<div class="output stream stdout">
<pre><code>
Training SVM with Linear Kernel...

Training SVM with Poly Kernel...

Training SVM with Rbf Kernel...
</code></pre>
</div>
</div>
<section id="step-3-evaluating-the-kernels" class="cell markdown" id="9XEbbZlFoYsj">
<h3><strong>Step 3: Evaluating The Kernels</strong></h3>
<p>This below code helps to evaluates the performance of the trained SVM models on the test dataset. Metrics like accuracy, precision, recall, and F1-score are calculated and displayed for each kernel.</p>
</section>
<div class="cell code" data-execution_count="3" data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}" id="kGQCbOvvVfnA" data-outputId="cdafa763-669e-4e1c-c4fa-d50f87d719c1">
<div class="sourceCode" id="cb5"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> accuracy_score, classification_report</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true"></a></span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true"></a><span class="co"># Define evaluation function</span></span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true"></a><span class="kw">def</span> evaluate_svm(model, kernel_name):</span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true"></a>    <span class="co">&quot;&quot;&quot;</span></span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true"></a><span class="co">    Evaluates a trained SVM model and prints performance metrics.</span></span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true"></a></span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true"></a><span class="co">    Parameters:</span></span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true"></a><span class="co">    - model (SVC): Trained SVM model.</span></span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true"></a><span class="co">    - kernel_name (str): Kernel name (for labeling).</span></span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true"></a></span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true"></a><span class="co">    Returns:</span></span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true"></a><span class="co">    - accuracy (float): Accuracy of the model on the test set.</span></span>
<span id="cb5-14"><a href="#cb5-14" aria-hidden="true"></a><span class="co">    &quot;&quot;&quot;</span></span>
<span id="cb5-15"><a href="#cb5-15" aria-hidden="true"></a>    y_pred <span class="op">=</span> model.predict(X_test)</span>
<span id="cb5-16"><a href="#cb5-16" aria-hidden="true"></a>    accuracy <span class="op">=</span> accuracy_score(y_test, y_pred)</span>
<span id="cb5-17"><a href="#cb5-17" aria-hidden="true"></a>    <span class="bu">print</span>(<span class="ss">f&quot;</span><span class="ch">\n</span><span class="ss">Classification Report for </span><span class="sc">{</span>kernel_name<span class="sc">.</span>capitalize()<span class="sc">}</span><span class="ss"> Kernel:&quot;</span>)</span>
<span id="cb5-18"><a href="#cb5-18" aria-hidden="true"></a>    <span class="bu">print</span>(classification_report(y_test, y_pred))</span>
<span id="cb5-19"><a href="#cb5-19" aria-hidden="true"></a>    <span class="cf">return</span> accuracy</span>
<span id="cb5-20"><a href="#cb5-20" aria-hidden="true"></a></span>
<span id="cb5-21"><a href="#cb5-21" aria-hidden="true"></a><span class="co"># Evaluate all trained models</span></span>
<span id="cb5-22"><a href="#cb5-22" aria-hidden="true"></a>results <span class="op">=</span> []</span>
<span id="cb5-23"><a href="#cb5-23" aria-hidden="true"></a><span class="cf">for</span> kernel, model <span class="kw">in</span> trained_models.items():</span>
<span id="cb5-24"><a href="#cb5-24" aria-hidden="true"></a>    <span class="bu">print</span>(<span class="ss">f&quot;</span><span class="ch">\n</span><span class="ss">Evaluating SVM with </span><span class="sc">{</span>kernel<span class="sc">.</span>capitalize()<span class="sc">}</span><span class="ss"> Kernel...&quot;</span>)</span>
<span id="cb5-25"><a href="#cb5-25" aria-hidden="true"></a>    accuracy <span class="op">=</span> evaluate_svm(model, kernel)</span>
<span id="cb5-26"><a href="#cb5-26" aria-hidden="true"></a>    results.append({<span class="st">&quot;Kernel&quot;</span>: kernel.capitalize(), <span class="st">&quot;Accuracy&quot;</span>: accuracy})</span>
<span id="cb5-27"><a href="#cb5-27" aria-hidden="true"></a></span>
<span id="cb5-28"><a href="#cb5-28" aria-hidden="true"></a><span class="co"># Summary of results</span></span>
<span id="cb5-29"><a href="#cb5-29" aria-hidden="true"></a><span class="bu">print</span>(<span class="st">&quot;</span><span class="ch">\n</span><span class="st">Summary of Kernel Performance:&quot;</span>)</span>
<span id="cb5-30"><a href="#cb5-30" aria-hidden="true"></a><span class="cf">for</span> result <span class="kw">in</span> results:</span>
<span id="cb5-31"><a href="#cb5-31" aria-hidden="true"></a>    <span class="bu">print</span>(<span class="ss">f&quot;Kernel: </span><span class="sc">{</span>result[<span class="st">&#39;Kernel&#39;</span>]<span class="sc">}</span><span class="ss">, Accuracy: </span><span class="sc">{</span>result[<span class="st">&#39;Accuracy&#39;</span>]<span class="sc">:.2f}</span><span class="ss">&quot;</span>)</span></code></pre></div>
<div class="output stream stdout">
<pre><code>
Evaluating SVM with Linear Kernel...

Classification Report for Linear Kernel:
              precision    recall  f1-score   support

           0       1.00      1.00      1.00        19
           1       0.54      0.54      0.54        13
           2       0.54      0.54      0.54        13

    accuracy                           0.73        45
   macro avg       0.69      0.69      0.69        45
weighted avg       0.73      0.73      0.73        45


Evaluating SVM with Poly Kernel...

Classification Report for Poly Kernel:
              precision    recall  f1-score   support

           0       1.00      1.00      1.00        19
           1       0.55      0.85      0.67        13
           2       0.67      0.31      0.42        13

    accuracy                           0.76        45
   macro avg       0.74      0.72      0.70        45
weighted avg       0.77      0.76      0.74        45


Evaluating SVM with Rbf Kernel...

Classification Report for Rbf Kernel:
              precision    recall  f1-score   support

           0       1.00      1.00      1.00        19
           1       0.54      0.54      0.54        13
           2       0.54      0.54      0.54        13

    accuracy                           0.73        45
   macro avg       0.69      0.69      0.69        45
weighted avg       0.73      0.73      0.73        45


Summary of Kernel Performance:
Kernel: Linear, Accuracy: 0.73
Kernel: Poly, Accuracy: 0.76
Kernel: Rbf, Accuracy: 0.73
</code></pre>
</div>
</div>
<section id="step-4-visualize-decision-boundaries" class="cell markdown" id="iXTiqGgLfY8W">
<h3><strong>Step 4: Visualize Decision Boundaries</strong></h3>
<p>Next we are going to creates an interactive plot to visualize the decision boundaries of SVM models trained with different kernels. It allows users to experiment with hyperparameters such as 𝐶 , degree (for Polynomial), and 𝛾 (for RBF).</p>
</section>
<div class="cell code" data-execution_count="4" data-colab="{&quot;height&quot;:697,&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;,&quot;referenced_widgets&quot;:[&quot;68fbaa5af56642949f501d72f00e9fea&quot;,&quot;ce1ee05243764caa9062deb04c7bfd65&quot;,&quot;363188076b864163be995e015a8642ac&quot;,&quot;18be5eacaa7e4b739ade908d958d81a5&quot;,&quot;69498877df994dcebbabf84da9f17932&quot;,&quot;fc91e5c9302f4820acaaac5232c4b817&quot;,&quot;049cdae05b8249b8affa075cfe42ea0c&quot;,&quot;0ad7aebd57d642c182e37cbb013902af&quot;,&quot;9621bfd6534b428db065b6b89fa33bb1&quot;,&quot;87facc11422b458b8bfd9fbe535a98cb&quot;,&quot;21161083f3d4416596ad77390ce2d3aa&quot;,&quot;4e78b7ea85684f91b7a2fbf34df4d688&quot;,&quot;522818ae6bf84ecea664965ff2918cf0&quot;,&quot;708aacb82a884a1eaef085ca4d4f7f07&quot;,&quot;c59aff2f7bd04e2aa20256c66d1d5fd8&quot;,&quot;ea4ccc92983c46549c8d3c4a30e202cb&quot;]}" id="i1e593K5fgL2" data-outputId="e09398a0-b8e3-4ef3-fb57-c7a432d67605">
<div class="sourceCode" id="cb7"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true"></a><span class="im">from</span> ipywidgets <span class="im">import</span> interact, Dropdown, FloatSlider, IntSlider</span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true"></a><span class="im">from</span> sklearn.svm <span class="im">import</span> SVC</span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true"></a></span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true"></a>sns.set_palette(<span class="st">&quot;colorblind&quot;</span>)  <span class="co"># Color-blind friendly palette</span></span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true"></a></span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true"></a><span class="co"># Create a meshgrid for visualization</span></span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true"></a><span class="kw">def</span> create_meshgrid(X):</span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true"></a>    x_min, x_max <span class="op">=</span> X[:, <span class="dv">0</span>].<span class="bu">min</span>() <span class="op">-</span> <span class="dv">1</span>, X[:, <span class="dv">0</span>].<span class="bu">max</span>() <span class="op">+</span> <span class="dv">1</span></span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true"></a>    y_min, y_max <span class="op">=</span> X[:, <span class="dv">1</span>].<span class="bu">min</span>() <span class="op">-</span> <span class="dv">1</span>, X[:, <span class="dv">1</span>].<span class="bu">max</span>() <span class="op">+</span> <span class="dv">1</span></span>
<span id="cb7-13"><a href="#cb7-13" aria-hidden="true"></a>    xx, yy <span class="op">=</span> np.meshgrid(np.linspace(x_min, x_max, <span class="dv">100</span>), np.linspace(y_min, y_max, <span class="dv">100</span>))</span>
<span id="cb7-14"><a href="#cb7-14" aria-hidden="true"></a>    <span class="cf">return</span> xx, yy</span>
<span id="cb7-15"><a href="#cb7-15" aria-hidden="true"></a></span>
<span id="cb7-16"><a href="#cb7-16" aria-hidden="true"></a>xx, yy <span class="op">=</span> create_meshgrid(X)</span>
<span id="cb7-17"><a href="#cb7-17" aria-hidden="true"></a></span>
<span id="cb7-18"><a href="#cb7-18" aria-hidden="true"></a><span class="co"># Interactive Decision Boundary Plot Function</span></span>
<span id="cb7-19"><a href="#cb7-19" aria-hidden="true"></a><span class="kw">def</span> plot_interactive_decision_boundary(kernel<span class="op">=</span><span class="st">&#39;linear&#39;</span>, degree<span class="op">=</span><span class="dv">3</span>, gamma<span class="op">=</span><span class="fl">0.5</span>, C<span class="op">=</span><span class="fl">1.0</span>):</span>
<span id="cb7-20"><a href="#cb7-20" aria-hidden="true"></a>    <span class="co">&quot;&quot;&quot;</span></span>
<span id="cb7-21"><a href="#cb7-21" aria-hidden="true"></a><span class="co">    Interactive function to visualize SVM decision boundaries with enhanced visibility.</span></span>
<span id="cb7-22"><a href="#cb7-22" aria-hidden="true"></a></span>
<span id="cb7-23"><a href="#cb7-23" aria-hidden="true"></a><span class="co">    Parameters:</span></span>
<span id="cb7-24"><a href="#cb7-24" aria-hidden="true"></a><span class="co">    - kernel (str): Kernel type (&#39;linear&#39;, &#39;poly&#39;, &#39;rbf&#39;).</span></span>
<span id="cb7-25"><a href="#cb7-25" aria-hidden="true"></a><span class="co">    - degree (int): Degree for polynomial kernel.</span></span>
<span id="cb7-26"><a href="#cb7-26" aria-hidden="true"></a><span class="co">    - gamma (float): Gamma for RBF kernel.</span></span>
<span id="cb7-27"><a href="#cb7-27" aria-hidden="true"></a><span class="co">    - C (float): Regularization parameter.</span></span>
<span id="cb7-28"><a href="#cb7-28" aria-hidden="true"></a></span>
<span id="cb7-29"><a href="#cb7-29" aria-hidden="true"></a><span class="co">    Returns:</span></span>
<span id="cb7-30"><a href="#cb7-30" aria-hidden="true"></a><span class="co">    - None</span></span>
<span id="cb7-31"><a href="#cb7-31" aria-hidden="true"></a><span class="co">    &quot;&quot;&quot;</span></span>
<span id="cb7-32"><a href="#cb7-32" aria-hidden="true"></a>    <span class="co"># Set kernel-specific parameters</span></span>
<span id="cb7-33"><a href="#cb7-33" aria-hidden="true"></a>    params <span class="op">=</span> {<span class="st">&#39;kernel&#39;</span>: kernel, <span class="st">&#39;C&#39;</span>: C}</span>
<span id="cb7-34"><a href="#cb7-34" aria-hidden="true"></a>    <span class="cf">if</span> kernel <span class="op">==</span> <span class="st">&#39;poly&#39;</span>:</span>
<span id="cb7-35"><a href="#cb7-35" aria-hidden="true"></a>        params[<span class="st">&#39;degree&#39;</span>] <span class="op">=</span> degree</span>
<span id="cb7-36"><a href="#cb7-36" aria-hidden="true"></a>    <span class="cf">if</span> kernel <span class="op">==</span> <span class="st">&#39;rbf&#39;</span>:</span>
<span id="cb7-37"><a href="#cb7-37" aria-hidden="true"></a>        params[<span class="st">&#39;gamma&#39;</span>] <span class="op">=</span> gamma</span>
<span id="cb7-38"><a href="#cb7-38" aria-hidden="true"></a></span>
<span id="cb7-39"><a href="#cb7-39" aria-hidden="true"></a>    <span class="co"># Train the SVM model</span></span>
<span id="cb7-40"><a href="#cb7-40" aria-hidden="true"></a>    model <span class="op">=</span> SVC(random_state<span class="op">=</span><span class="dv">42</span>, <span class="op">**</span>params)</span>
<span id="cb7-41"><a href="#cb7-41" aria-hidden="true"></a>    model.fit(X_train, y_train)</span>
<span id="cb7-42"><a href="#cb7-42" aria-hidden="true"></a></span>
<span id="cb7-43"><a href="#cb7-43" aria-hidden="true"></a>    <span class="co"># Predict class labels for the meshgrid</span></span>
<span id="cb7-44"><a href="#cb7-44" aria-hidden="true"></a>    Z <span class="op">=</span> model.predict(np.c_[xx.ravel(), yy.ravel()])</span>
<span id="cb7-45"><a href="#cb7-45" aria-hidden="true"></a>    Z <span class="op">=</span> Z.reshape(xx.shape)</span>
<span id="cb7-46"><a href="#cb7-46" aria-hidden="true"></a></span>
<span id="cb7-47"><a href="#cb7-47" aria-hidden="true"></a>    <span class="co"># Plot the decision boundary</span></span>
<span id="cb7-48"><a href="#cb7-48" aria-hidden="true"></a>    plt.figure(figsize<span class="op">=</span>(<span class="dv">8</span>,<span class="dv">6</span>))</span>
<span id="cb7-49"><a href="#cb7-49" aria-hidden="true"></a>    markers <span class="op">=</span> [<span class="st">&#39;o&#39;</span>, <span class="st">&#39;^&#39;</span>, <span class="st">&#39;s&#39;</span>]</span>
<span id="cb7-50"><a href="#cb7-50" aria-hidden="true"></a>    colors <span class="op">=</span> sns.color_palette(<span class="st">&quot;colorblind&quot;</span>)[:<span class="bu">len</span>(np.unique(y))]</span>
<span id="cb7-51"><a href="#cb7-51" aria-hidden="true"></a></span>
<span id="cb7-52"><a href="#cb7-52" aria-hidden="true"></a>    <span class="co"># Plot decision boundary</span></span>
<span id="cb7-53"><a href="#cb7-53" aria-hidden="true"></a>    plt.contourf(xx, yy, Z, alpha<span class="op">=</span><span class="fl">0.8</span>, cmap<span class="op">=</span><span class="st">&#39;Blues&#39;</span>)  <span class="co"># Reduced alpha for better point visibility</span></span>
<span id="cb7-54"><a href="#cb7-54" aria-hidden="true"></a></span>
<span id="cb7-55"><a href="#cb7-55" aria-hidden="true"></a>    <span class="co"># Plot class points</span></span>
<span id="cb7-56"><a href="#cb7-56" aria-hidden="true"></a>    <span class="cf">for</span> class_label, color, marker <span class="kw">in</span> <span class="bu">zip</span>(np.unique(y), colors, markers):</span>
<span id="cb7-57"><a href="#cb7-57" aria-hidden="true"></a>        plt.scatter(</span>
<span id="cb7-58"><a href="#cb7-58" aria-hidden="true"></a>            X[y <span class="op">==</span> class_label][:, <span class="dv">0</span>],</span>
<span id="cb7-59"><a href="#cb7-59" aria-hidden="true"></a>            X[y <span class="op">==</span> class_label][:, <span class="dv">1</span>],</span>
<span id="cb7-60"><a href="#cb7-60" aria-hidden="true"></a>            color<span class="op">=</span>color,</span>
<span id="cb7-61"><a href="#cb7-61" aria-hidden="true"></a>            marker<span class="op">=</span>marker,</span>
<span id="cb7-62"><a href="#cb7-62" aria-hidden="true"></a>            edgecolor<span class="op">=</span><span class="st">&#39;black&#39;</span>,  <span class="co"># Black edges for better contrast</span></span>
<span id="cb7-63"><a href="#cb7-63" aria-hidden="true"></a>            label<span class="op">=</span><span class="ss">f&quot;Class </span><span class="sc">{</span>class_label<span class="sc">}</span><span class="ss">&quot;</span>,</span>
<span id="cb7-64"><a href="#cb7-64" aria-hidden="true"></a>            s<span class="op">=</span><span class="dv">50</span>  <span class="co"># Increased size</span></span>
<span id="cb7-65"><a href="#cb7-65" aria-hidden="true"></a>        )</span>
<span id="cb7-66"><a href="#cb7-66" aria-hidden="true"></a></span>
<span id="cb7-67"><a href="#cb7-67" aria-hidden="true"></a>    <span class="co"># Enhance plot details</span></span>
<span id="cb7-68"><a href="#cb7-68" aria-hidden="true"></a>    plt.title(<span class="ss">f&quot;SVM Decision Boundary (</span><span class="sc">{</span>kernel<span class="sc">.</span>capitalize()<span class="sc">}</span><span class="ss"> Kernel)&quot;</span>, fontsize<span class="op">=</span><span class="dv">14</span>)</span>
<span id="cb7-69"><a href="#cb7-69" aria-hidden="true"></a>    plt.xlabel(<span class="st">&quot;Feature 1 (Standardized)&quot;</span>, fontsize<span class="op">=</span><span class="dv">12</span>)</span>
<span id="cb7-70"><a href="#cb7-70" aria-hidden="true"></a>    plt.ylabel(<span class="st">&quot;Feature 2 (Standardized)&quot;</span>, fontsize<span class="op">=</span><span class="dv">12</span>)</span>
<span id="cb7-71"><a href="#cb7-71" aria-hidden="true"></a>    plt.legend(title<span class="op">=</span><span class="st">&quot;Classes&quot;</span>, loc<span class="op">=</span><span class="st">&quot;upper left&quot;</span>, fontsize<span class="op">=</span><span class="dv">10</span>)</span>
<span id="cb7-72"><a href="#cb7-72" aria-hidden="true"></a>    plt.grid(alpha<span class="op">=</span><span class="fl">0.5</span>)</span>
<span id="cb7-73"><a href="#cb7-73" aria-hidden="true"></a>    plt.show()</span>
<span id="cb7-74"><a href="#cb7-74" aria-hidden="true"></a></span>
<span id="cb7-75"><a href="#cb7-75" aria-hidden="true"></a><span class="co"># Interactive widgets with refined ranges and defaults</span></span>
<span id="cb7-76"><a href="#cb7-76" aria-hidden="true"></a><span class="kw">def</span> interactive_plot():</span>
<span id="cb7-77"><a href="#cb7-77" aria-hidden="true"></a>    interact(</span>
<span id="cb7-78"><a href="#cb7-78" aria-hidden="true"></a>        plot_interactive_decision_boundary,</span>
<span id="cb7-79"><a href="#cb7-79" aria-hidden="true"></a>        kernel<span class="op">=</span>Dropdown(options<span class="op">=</span>[<span class="st">&#39;linear&#39;</span>, <span class="st">&#39;poly&#39;</span>, <span class="st">&#39;rbf&#39;</span>], description<span class="op">=</span><span class="st">&quot;Kernel&quot;</span>),</span>
<span id="cb7-80"><a href="#cb7-80" aria-hidden="true"></a>        degree<span class="op">=</span>IntSlider(<span class="bu">min</span><span class="op">=</span><span class="dv">2</span>, <span class="bu">max</span><span class="op">=</span><span class="dv">10</span>, step<span class="op">=</span><span class="dv">1</span>, value<span class="op">=</span><span class="dv">3</span>, description<span class="op">=</span><span class="st">&quot;Degree (Poly)&quot;</span>),</span>
<span id="cb7-81"><a href="#cb7-81" aria-hidden="true"></a>        gamma<span class="op">=</span>FloatSlider(<span class="bu">min</span><span class="op">=</span><span class="fl">0.01</span>, <span class="bu">max</span><span class="op">=</span><span class="fl">1.0</span>, step<span class="op">=</span><span class="fl">0.05</span>, value<span class="op">=</span><span class="fl">0.5</span>, description<span class="op">=</span><span class="st">&quot;Gamma (RBF)&quot;</span>),</span>
<span id="cb7-82"><a href="#cb7-82" aria-hidden="true"></a>        C<span class="op">=</span>FloatSlider(<span class="bu">min</span><span class="op">=</span><span class="fl">0.01</span>, <span class="bu">max</span><span class="op">=</span><span class="fl">10.0</span>, step<span class="op">=</span><span class="fl">0.1</span>, value<span class="op">=</span><span class="fl">1.0</span>, description<span class="op">=</span><span class="st">&quot;C (Regularization)&quot;</span>)</span>
<span id="cb7-83"><a href="#cb7-83" aria-hidden="true"></a>    )</span>
<span id="cb7-84"><a href="#cb7-84" aria-hidden="true"></a></span>
<span id="cb7-85"><a href="#cb7-85" aria-hidden="true"></a><span class="co"># Call the interactive plot function</span></span>
<span id="cb7-86"><a href="#cb7-86" aria-hidden="true"></a>interactive_plot()</span></code></pre></div>
<div class="output display_data">
<div class="sourceCode" id="cb8"><pre class="sourceCode json"><code class="sourceCode json"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true"></a><span class="fu">{</span><span class="dt">&quot;model_id&quot;</span><span class="fu">:</span><span class="st">&quot;68fbaa5af56642949f501d72f00e9fea&quot;</span><span class="fu">,</span><span class="dt">&quot;version_major&quot;</span><span class="fu">:</span><span class="dv">2</span><span class="fu">,</span><span class="dt">&quot;version_minor&quot;</span><span class="fu">:</span><span class="dv">0</span><span class="fu">}</span></span></code></pre></div>
</div>
</div>
<div class="cell markdown" id="jqhAAgsIFdFL">
<p>The plot above demonstrates the decision boundary created by an SVM model. Here’s what the visualization highlights:</p>
<ul>
<li>Blue Circles (Class 0), Orange Triangles (Class 1), and Green Squares (Class 2): Represent the three classes in the dataset.</li>
</ul>
</div>
<section id="step-5-kernel-comparisons" class="cell markdown" id="P5tzK71caG5A">
<h3><strong>Step 5: Kernel Comparisons</strong></h3>
<p>This kernel comparison code that builds on the interactive plot to evaluate and compare the performance of different SVM kernels across multiple metrics.</p>
<p>This code includes:</p>
<p>Kernel evaluation: Accuracy, precision, recall, F1-score. Visualization: Plots decision boundaries for each kernel.</p>
</section>
<div class="cell code" data-execution_count="5" data-colab="{&quot;height&quot;:1000,&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}" id="-eqwoZXBZ9eQ" data-outputId="9d5d5f55-c1d3-49ab-ad0f-14d7f8d30ce4">
<div class="sourceCode" id="cb9"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true"></a><span class="im">from</span> sklearn.svm <span class="im">import</span> SVC</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> accuracy_score</span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true"></a></span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true"></a><span class="co"># Set color-blind-friendly palette</span></span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true"></a>sns.set_palette(<span class="st">&quot;colorblind&quot;</span>)</span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true"></a></span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true"></a><span class="kw">def</span> plot_decision_boundary(model, kernel_name, X_train, y_train, save_as<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb9-11"><a href="#cb9-11" aria-hidden="true"></a>    <span class="co">&quot;&quot;&quot;</span></span>
<span id="cb9-12"><a href="#cb9-12" aria-hidden="true"></a><span class="co">    Plots the decision boundary for an SVM model with a consistent color scheme.</span></span>
<span id="cb9-13"><a href="#cb9-13" aria-hidden="true"></a></span>
<span id="cb9-14"><a href="#cb9-14" aria-hidden="true"></a><span class="co">    Parameters:</span></span>
<span id="cb9-15"><a href="#cb9-15" aria-hidden="true"></a><span class="co">        model (SVC): Trained SVM model.</span></span>
<span id="cb9-16"><a href="#cb9-16" aria-hidden="true"></a><span class="co">        kernel_name (str): Name of the kernel for labeling the plot.</span></span>
<span id="cb9-17"><a href="#cb9-17" aria-hidden="true"></a><span class="co">        X_train (ndarray): Training feature data.</span></span>
<span id="cb9-18"><a href="#cb9-18" aria-hidden="true"></a><span class="co">        y_train (ndarray): Training labels.</span></span>
<span id="cb9-19"><a href="#cb9-19" aria-hidden="true"></a><span class="co">        save_as (str): File name to save the plot (optional).</span></span>
<span id="cb9-20"><a href="#cb9-20" aria-hidden="true"></a><span class="co">    &quot;&quot;&quot;</span></span>
<span id="cb9-21"><a href="#cb9-21" aria-hidden="true"></a>    <span class="co"># Create meshgrid</span></span>
<span id="cb9-22"><a href="#cb9-22" aria-hidden="true"></a>    x_min, x_max <span class="op">=</span> X_train[:, <span class="dv">0</span>].<span class="bu">min</span>() <span class="op">-</span> <span class="dv">1</span>, X_train[:, <span class="dv">0</span>].<span class="bu">max</span>() <span class="op">+</span> <span class="dv">1</span></span>
<span id="cb9-23"><a href="#cb9-23" aria-hidden="true"></a>    y_min, y_max <span class="op">=</span> X_train[:, <span class="dv">1</span>].<span class="bu">min</span>() <span class="op">-</span> <span class="dv">1</span>, X_train[:, <span class="dv">1</span>].<span class="bu">max</span>() <span class="op">+</span> <span class="dv">1</span></span>
<span id="cb9-24"><a href="#cb9-24" aria-hidden="true"></a>    xx, yy <span class="op">=</span> np.meshgrid(np.linspace(x_min, x_max, <span class="dv">100</span>),</span>
<span id="cb9-25"><a href="#cb9-25" aria-hidden="true"></a>                         np.linspace(y_min, y_max, <span class="dv">100</span>))</span>
<span id="cb9-26"><a href="#cb9-26" aria-hidden="true"></a></span>
<span id="cb9-27"><a href="#cb9-27" aria-hidden="true"></a>    <span class="co"># Predict on meshgrid</span></span>
<span id="cb9-28"><a href="#cb9-28" aria-hidden="true"></a>    Z <span class="op">=</span> model.predict(np.c_[xx.ravel(), yy.ravel()])</span>
<span id="cb9-29"><a href="#cb9-29" aria-hidden="true"></a>    Z <span class="op">=</span> Z.reshape(xx.shape)</span>
<span id="cb9-30"><a href="#cb9-30" aria-hidden="true"></a></span>
<span id="cb9-31"><a href="#cb9-31" aria-hidden="true"></a>    <span class="co"># Plot decision boundary</span></span>
<span id="cb9-32"><a href="#cb9-32" aria-hidden="true"></a>    plt.figure(figsize<span class="op">=</span>(<span class="dv">8</span>, <span class="dv">6</span>))</span>
<span id="cb9-33"><a href="#cb9-33" aria-hidden="true"></a>    plt.contourf(xx, yy, Z, alpha<span class="op">=</span><span class="fl">0.8</span>, cmap<span class="op">=</span><span class="st">&#39;Blues&#39;</span>)  <span class="co"># Consistent colormap</span></span>
<span id="cb9-34"><a href="#cb9-34" aria-hidden="true"></a>    unique_classes <span class="op">=</span> np.unique(y_train)</span>
<span id="cb9-35"><a href="#cb9-35" aria-hidden="true"></a></span>
<span id="cb9-36"><a href="#cb9-36" aria-hidden="true"></a>    <span class="co"># Define consistent color and marker for all plots</span></span>
<span id="cb9-37"><a href="#cb9-37" aria-hidden="true"></a>    consistent_color <span class="op">=</span> sns.color_palette(<span class="st">&quot;colorblind&quot;</span>)[:<span class="bu">len</span>(unique_classes)]</span>
<span id="cb9-38"><a href="#cb9-38" aria-hidden="true"></a>    markers <span class="op">=</span> [<span class="st">&#39;o&#39;</span>, <span class="st">&#39;^&#39;</span>, <span class="st">&#39;s&#39;</span>]  <span class="co"># Same markers for all classes</span></span>
<span id="cb9-39"><a href="#cb9-39" aria-hidden="true"></a></span>
<span id="cb9-40"><a href="#cb9-40" aria-hidden="true"></a>    <span class="cf">for</span> class_label, color, marker <span class="kw">in</span> <span class="bu">zip</span>(unique_classes, consistent_color, markers):</span>
<span id="cb9-41"><a href="#cb9-41" aria-hidden="true"></a>        plt.scatter(</span>
<span id="cb9-42"><a href="#cb9-42" aria-hidden="true"></a>            X_train[y_train <span class="op">==</span> class_label][:, <span class="dv">0</span>],</span>
<span id="cb9-43"><a href="#cb9-43" aria-hidden="true"></a>            X_train[y_train <span class="op">==</span> class_label][:, <span class="dv">1</span>],</span>
<span id="cb9-44"><a href="#cb9-44" aria-hidden="true"></a>            marker<span class="op">=</span>marker,</span>
<span id="cb9-45"><a href="#cb9-45" aria-hidden="true"></a>            color<span class="op">=</span>color,</span>
<span id="cb9-46"><a href="#cb9-46" aria-hidden="true"></a>            edgecolor<span class="op">=</span><span class="st">&#39;k&#39;</span>,</span>
<span id="cb9-47"><a href="#cb9-47" aria-hidden="true"></a>            label<span class="op">=</span><span class="ss">f&quot;Class </span><span class="sc">{</span>class_label<span class="sc">}</span><span class="ss">&quot;</span>,</span>
<span id="cb9-48"><a href="#cb9-48" aria-hidden="true"></a>            s<span class="op">=</span><span class="dv">50</span></span>
<span id="cb9-49"><a href="#cb9-49" aria-hidden="true"></a>        )</span>
<span id="cb9-50"><a href="#cb9-50" aria-hidden="true"></a>    plt.title(<span class="ss">f&quot;SVM Decision Boundary (</span><span class="sc">{</span>kernel_name<span class="sc">}</span><span class="ss">)&quot;</span>)</span>
<span id="cb9-51"><a href="#cb9-51" aria-hidden="true"></a>    plt.xlabel(<span class="st">&quot;Feature 1 (Standardized)&quot;</span>)</span>
<span id="cb9-52"><a href="#cb9-52" aria-hidden="true"></a>    plt.ylabel(<span class="st">&quot;Feature 2 (Standardized)&quot;</span>)</span>
<span id="cb9-53"><a href="#cb9-53" aria-hidden="true"></a>    plt.legend(title<span class="op">=</span><span class="st">&quot;Classes&quot;</span>, loc<span class="op">=</span><span class="st">&quot;upper right&quot;</span>)</span>
<span id="cb9-54"><a href="#cb9-54" aria-hidden="true"></a>    <span class="cf">if</span> save_as:</span>
<span id="cb9-55"><a href="#cb9-55" aria-hidden="true"></a>        plt.savefig(save_as)</span>
<span id="cb9-56"><a href="#cb9-56" aria-hidden="true"></a>    plt.show()</span>
<span id="cb9-57"><a href="#cb9-57" aria-hidden="true"></a></span>
<span id="cb9-58"><a href="#cb9-58" aria-hidden="true"></a><span class="kw">def</span> compare_kernels(X_train, y_train, X_test, y_test, kernels, params_list):</span>
<span id="cb9-59"><a href="#cb9-59" aria-hidden="true"></a>    <span class="co">&quot;&quot;&quot;</span></span>
<span id="cb9-60"><a href="#cb9-60" aria-hidden="true"></a><span class="co">    Compares different SVM kernels by training models, calculating accuracy,</span></span>
<span id="cb9-61"><a href="#cb9-61" aria-hidden="true"></a><span class="co">    and visualizing decision boundaries.</span></span>
<span id="cb9-62"><a href="#cb9-62" aria-hidden="true"></a></span>
<span id="cb9-63"><a href="#cb9-63" aria-hidden="true"></a><span class="co">    Parameters:</span></span>
<span id="cb9-64"><a href="#cb9-64" aria-hidden="true"></a><span class="co">        X_train (ndarray): Training feature data.</span></span>
<span id="cb9-65"><a href="#cb9-65" aria-hidden="true"></a><span class="co">        y_train (ndarray): Training labels.</span></span>
<span id="cb9-66"><a href="#cb9-66" aria-hidden="true"></a><span class="co">        X_test (ndarray): Testing feature data.</span></span>
<span id="cb9-67"><a href="#cb9-67" aria-hidden="true"></a><span class="co">        y_test (ndarray): Testing labels.</span></span>
<span id="cb9-68"><a href="#cb9-68" aria-hidden="true"></a><span class="co">        kernels (list of str): List of kernel names (&#39;linear&#39;, &#39;poly&#39;, &#39;rbf&#39;, etc.).</span></span>
<span id="cb9-69"><a href="#cb9-69" aria-hidden="true"></a><span class="co">        params_list (list of dict): List of parameters for each kernel.</span></span>
<span id="cb9-70"><a href="#cb9-70" aria-hidden="true"></a></span>
<span id="cb9-71"><a href="#cb9-71" aria-hidden="true"></a><span class="co">    Returns:</span></span>
<span id="cb9-72"><a href="#cb9-72" aria-hidden="true"></a><span class="co">        results (list of dict): List of dictionaries containing kernel name and accuracy.</span></span>
<span id="cb9-73"><a href="#cb9-73" aria-hidden="true"></a><span class="co">    &quot;&quot;&quot;</span></span>
<span id="cb9-74"><a href="#cb9-74" aria-hidden="true"></a>    results <span class="op">=</span> []</span>
<span id="cb9-75"><a href="#cb9-75" aria-hidden="true"></a>    <span class="cf">for</span> kernel, params <span class="kw">in</span> <span class="bu">zip</span>(kernels, params_list):</span>
<span id="cb9-76"><a href="#cb9-76" aria-hidden="true"></a>        <span class="bu">print</span>(<span class="ss">f&quot;</span><span class="ch">\n</span><span class="ss">--- Evaluating </span><span class="sc">{</span>kernel<span class="sc">.</span>capitalize()<span class="sc">}</span><span class="ss"> Kernel ---&quot;</span>)</span>
<span id="cb9-77"><a href="#cb9-77" aria-hidden="true"></a></span>
<span id="cb9-78"><a href="#cb9-78" aria-hidden="true"></a>        <span class="co"># Train the SVM model</span></span>
<span id="cb9-79"><a href="#cb9-79" aria-hidden="true"></a>        params <span class="op">=</span> params <span class="cf">if</span> params <span class="cf">else</span> {}</span>
<span id="cb9-80"><a href="#cb9-80" aria-hidden="true"></a>        model <span class="op">=</span> SVC(kernel<span class="op">=</span>kernel, random_state<span class="op">=</span><span class="dv">42</span>, <span class="op">**</span>params)</span>
<span id="cb9-81"><a href="#cb9-81" aria-hidden="true"></a>        model.fit(X_train, y_train)</span>
<span id="cb9-82"><a href="#cb9-82" aria-hidden="true"></a></span>
<span id="cb9-83"><a href="#cb9-83" aria-hidden="true"></a>        <span class="co"># Predict on the test set</span></span>
<span id="cb9-84"><a href="#cb9-84" aria-hidden="true"></a>        y_pred <span class="op">=</span> model.predict(X_test)</span>
<span id="cb9-85"><a href="#cb9-85" aria-hidden="true"></a></span>
<span id="cb9-86"><a href="#cb9-86" aria-hidden="true"></a>        <span class="co"># Evaluate accuracy</span></span>
<span id="cb9-87"><a href="#cb9-87" aria-hidden="true"></a>        accuracy <span class="op">=</span> accuracy_score(y_test, y_pred)</span>
<span id="cb9-88"><a href="#cb9-88" aria-hidden="true"></a>        results.append({<span class="st">&quot;Kernel&quot;</span>: kernel.capitalize(), <span class="st">&quot;Accuracy&quot;</span>: accuracy})</span>
<span id="cb9-89"><a href="#cb9-89" aria-hidden="true"></a></span>
<span id="cb9-90"><a href="#cb9-90" aria-hidden="true"></a>        <span class="co"># Visualize decision boundary</span></span>
<span id="cb9-91"><a href="#cb9-91" aria-hidden="true"></a>        plot_decision_boundary(model, kernel_name<span class="op">=</span><span class="ss">f&quot;</span><span class="sc">{</span>kernel<span class="sc">.</span>capitalize()<span class="sc">}</span><span class="ss"> Kernel&quot;</span>,</span>
<span id="cb9-92"><a href="#cb9-92" aria-hidden="true"></a>                               X_train<span class="op">=</span>X_train, y_train<span class="op">=</span>y_train)</span>
<span id="cb9-93"><a href="#cb9-93" aria-hidden="true"></a></span>
<span id="cb9-94"><a href="#cb9-94" aria-hidden="true"></a>    <span class="cf">return</span> results</span>
<span id="cb9-95"><a href="#cb9-95" aria-hidden="true"></a></span>
<span id="cb9-96"><a href="#cb9-96" aria-hidden="true"></a><span class="co"># Example usage:</span></span>
<span id="cb9-97"><a href="#cb9-97" aria-hidden="true"></a><span class="co"># Assuming X_train, y_train, X_test, y_test are defined</span></span>
<span id="cb9-98"><a href="#cb9-98" aria-hidden="true"></a>kernels <span class="op">=</span> [<span class="st">&#39;linear&#39;</span>, <span class="st">&#39;poly&#39;</span>, <span class="st">&#39;rbf&#39;</span>]</span>
<span id="cb9-99"><a href="#cb9-99" aria-hidden="true"></a>params_list <span class="op">=</span> [<span class="va">None</span>, {<span class="st">&#39;degree&#39;</span>: <span class="dv">3</span>}, {<span class="st">&#39;gamma&#39;</span>: <span class="fl">0.5</span>}]</span>
<span id="cb9-100"><a href="#cb9-100" aria-hidden="true"></a></span>
<span id="cb9-101"><a href="#cb9-101" aria-hidden="true"></a><span class="co"># Compare kernels and collect results</span></span>
<span id="cb9-102"><a href="#cb9-102" aria-hidden="true"></a>kernel_results <span class="op">=</span> compare_kernels(X_train, y_train, X_test, y_test, kernels, params_list)</span>
<span id="cb9-103"><a href="#cb9-103" aria-hidden="true"></a></span>
<span id="cb9-104"><a href="#cb9-104" aria-hidden="true"></a><span class="co"># Summary of results</span></span>
<span id="cb9-105"><a href="#cb9-105" aria-hidden="true"></a><span class="bu">print</span>(<span class="st">&quot;</span><span class="ch">\n</span><span class="st">Summary of Kernel Performance:&quot;</span>)</span>
<span id="cb9-106"><a href="#cb9-106" aria-hidden="true"></a><span class="cf">for</span> result <span class="kw">in</span> kernel_results:</span>
<span id="cb9-107"><a href="#cb9-107" aria-hidden="true"></a>    <span class="bu">print</span>(<span class="ss">f&quot;Kernel: </span><span class="sc">{</span>result[<span class="st">&#39;Kernel&#39;</span>]<span class="sc">}</span><span class="ss">, Accuracy: </span><span class="sc">{</span>result[<span class="st">&#39;Accuracy&#39;</span>]<span class="sc">:.2f}</span><span class="ss">&quot;</span>)</span>
<span id="cb9-108"><a href="#cb9-108" aria-hidden="true"></a></span></code></pre></div>
<div class="output stream stdout">
<pre><code>
--- Evaluating Linear Kernel ---
</code></pre>
</div>
<div class="output display_data">
<p><img src="204a280858b414c8a90faf7ade9f85f70c48b3c7.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
--- Evaluating Poly Kernel ---
</code></pre>
</div>
<div class="output display_data">
<p><img src="93eb8eade0b27e286871042cf49163ca5aaccd8b.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
--- Evaluating Rbf Kernel ---
</code></pre>
</div>
<div class="output display_data">
<p><img src="939dcedabe170108d8c5ff1bc52e6fb744c0bd44.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>
Summary of Kernel Performance:
Kernel: Linear, Accuracy: 0.73
Kernel: Poly, Accuracy: 0.76
Kernel: Rbf, Accuracy: 0.73
</code></pre>
</div>
</div>
<section id="step-6-hyperparameter-tuning-effect-of-𝐶" class="cell markdown" id="KlggTcN-Gab_">
<h3><strong>Step 6: Hyperparameter Tuning: Effect of 𝐶</strong></h3>
<p>This step focuses on plotting the relationship between the regularization parameter 𝐶 and the model's accuracy for both training and testing datasets. This step visualizes the effect of different 𝐶 values on the decision boundary and model performance, highlighting the trade-off between underfitting (low 𝐶</p>
<p>C) and overfitting (high 𝐶 C).</p>
<ul>
<li>Low 𝐶 : The model allows a larger margin and tolerates more misclassifications, leading to underfitting and lower accuracy.</li>
<li>High 𝐶 : The model creates a tighter margin, minimizing misclassification errors but risking overfitting to the training data.</li>
</ul>
<p>By presenting decision boundaries alongside accuracy trends, this step gives an intuitive and quantitative view of how tuning 𝐶 affects model performance.</p>
</section>
<div class="cell code" data-execution_count="6" data-colab="{&quot;height&quot;:1000,&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}" id="vrJgKHeFGwTO" data-outputId="40065028-1d42-4e4b-f083-ee1cb76c9719">
<div class="sourceCode" id="cb14"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true"></a><span class="im">from</span> sklearn.svm <span class="im">import</span> SVC</span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true"></a><span class="im">from</span> sklearn.datasets <span class="im">import</span> load_iris</span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true"></a></span>
<span id="cb14-8"><a href="#cb14-8" aria-hidden="true"></a><span class="co"># Set a colorblind-friendly palette</span></span>
<span id="cb14-9"><a href="#cb14-9" aria-hidden="true"></a>sns.set_palette(<span class="st">&quot;colorblind&quot;</span>)</span>
<span id="cb14-10"><a href="#cb14-10" aria-hidden="true"></a></span>
<span id="cb14-11"><a href="#cb14-11" aria-hidden="true"></a><span class="co"># Load the Iris dataset</span></span>
<span id="cb14-12"><a href="#cb14-12" aria-hidden="true"></a>iris <span class="op">=</span> load_iris()</span>
<span id="cb14-13"><a href="#cb14-13" aria-hidden="true"></a>X, y <span class="op">=</span> iris.data[:, :<span class="dv">2</span>], iris.target  <span class="co"># Using only the first two features for visualization</span></span>
<span id="cb14-14"><a href="#cb14-14" aria-hidden="true"></a></span>
<span id="cb14-15"><a href="#cb14-15" aria-hidden="true"></a><span class="co"># Split the dataset into training and testing subsets</span></span>
<span id="cb14-16"><a href="#cb14-16" aria-hidden="true"></a>X_train, X_test, y_train, y_test <span class="op">=</span> train_test_split(X, y, test_size<span class="op">=</span><span class="fl">0.3</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb14-17"><a href="#cb14-17" aria-hidden="true"></a></span>
<span id="cb14-18"><a href="#cb14-18" aria-hidden="true"></a><span class="co"># Define a range of C and gamma values to test</span></span>
<span id="cb14-19"><a href="#cb14-19" aria-hidden="true"></a>C_values <span class="op">=</span> [<span class="fl">0.001</span>, <span class="fl">0.01</span>, <span class="fl">0.1</span>, <span class="dv">1</span>, <span class="dv">10</span>, <span class="dv">100</span>]</span>
<span id="cb14-20"><a href="#cb14-20" aria-hidden="true"></a>gamma_values <span class="op">=</span> [<span class="fl">0.001</span>, <span class="fl">0.01</span>, <span class="fl">0.1</span>, <span class="dv">1</span>, <span class="dv">10</span>, <span class="dv">100</span>]</span>
<span id="cb14-21"><a href="#cb14-21" aria-hidden="true"></a></span>
<span id="cb14-22"><a href="#cb14-22" aria-hidden="true"></a><span class="co"># Initialize dictionaries to store train and test accuracies</span></span>
<span id="cb14-23"><a href="#cb14-23" aria-hidden="true"></a>results <span class="op">=</span> {<span class="st">&quot;train&quot;</span>: [], <span class="st">&quot;test&quot;</span>: []}</span>
<span id="cb14-24"><a href="#cb14-24" aria-hidden="true"></a></span>
<span id="cb14-25"><a href="#cb14-25" aria-hidden="true"></a><span class="co"># Plot decision boundaries for different C values</span></span>
<span id="cb14-26"><a href="#cb14-26" aria-hidden="true"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">15</span>, <span class="dv">10</span>))</span>
<span id="cb14-27"><a href="#cb14-27" aria-hidden="true"></a><span class="cf">for</span> i, (C, gamma) <span class="kw">in</span> <span class="bu">enumerate</span>(<span class="bu">zip</span>(C_values, gamma_values)):</span>
<span id="cb14-28"><a href="#cb14-28" aria-hidden="true"></a>    <span class="co"># Train SVM model with RBF kernel, varying C and gamma</span></span>
<span id="cb14-29"><a href="#cb14-29" aria-hidden="true"></a>    model <span class="op">=</span> SVC(kernel<span class="op">=</span><span class="st">&#39;rbf&#39;</span>, C<span class="op">=</span>C, gamma<span class="op">=</span>gamma, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb14-30"><a href="#cb14-30" aria-hidden="true"></a>    model.fit(X_train, y_train)</span>
<span id="cb14-31"><a href="#cb14-31" aria-hidden="true"></a></span>
<span id="cb14-32"><a href="#cb14-32" aria-hidden="true"></a>    <span class="co"># Calculate train and test accuracy</span></span>
<span id="cb14-33"><a href="#cb14-33" aria-hidden="true"></a>    train_acc <span class="op">=</span> model.score(X_train, y_train)</span>
<span id="cb14-34"><a href="#cb14-34" aria-hidden="true"></a>    test_acc <span class="op">=</span> model.score(X_test, y_test)</span>
<span id="cb14-35"><a href="#cb14-35" aria-hidden="true"></a>    results[<span class="st">&quot;train&quot;</span>].append(train_acc)</span>
<span id="cb14-36"><a href="#cb14-36" aria-hidden="true"></a>    results[<span class="st">&quot;test&quot;</span>].append(test_acc)</span>
<span id="cb14-37"><a href="#cb14-37" aria-hidden="true"></a></span>
<span id="cb14-38"><a href="#cb14-38" aria-hidden="true"></a>    <span class="co"># Create meshgrid for decision boundary visualization</span></span>
<span id="cb14-39"><a href="#cb14-39" aria-hidden="true"></a>    x_min, x_max <span class="op">=</span> X[:, <span class="dv">0</span>].<span class="bu">min</span>() <span class="op">-</span> <span class="dv">1</span>, X[:, <span class="dv">0</span>].<span class="bu">max</span>() <span class="op">+</span> <span class="dv">1</span></span>
<span id="cb14-40"><a href="#cb14-40" aria-hidden="true"></a>    y_min, y_max <span class="op">=</span> X[:, <span class="dv">1</span>].<span class="bu">min</span>() <span class="op">-</span> <span class="dv">1</span>, X[:, <span class="dv">1</span>].<span class="bu">max</span>() <span class="op">+</span> <span class="dv">1</span></span>
<span id="cb14-41"><a href="#cb14-41" aria-hidden="true"></a>    xx, yy <span class="op">=</span> np.meshgrid(np.linspace(x_min, x_max, <span class="dv">200</span>),</span>
<span id="cb14-42"><a href="#cb14-42" aria-hidden="true"></a>                         np.linspace(y_min, y_max, <span class="dv">200</span>))</span>
<span id="cb14-43"><a href="#cb14-43" aria-hidden="true"></a></span>
<span id="cb14-44"><a href="#cb14-44" aria-hidden="true"></a>    <span class="co"># Predict on the meshgrid</span></span>
<span id="cb14-45"><a href="#cb14-45" aria-hidden="true"></a>    Z <span class="op">=</span> model.predict(np.c_[xx.ravel(), yy.ravel()])</span>
<span id="cb14-46"><a href="#cb14-46" aria-hidden="true"></a>    Z <span class="op">=</span> Z.reshape(xx.shape)</span>
<span id="cb14-47"><a href="#cb14-47" aria-hidden="true"></a></span>
<span id="cb14-48"><a href="#cb14-48" aria-hidden="true"></a>    <span class="co"># Plot decision boundary</span></span>
<span id="cb14-49"><a href="#cb14-49" aria-hidden="true"></a>    plt.subplot(<span class="dv">2</span>, <span class="dv">3</span>, i <span class="op">+</span> <span class="dv">1</span>)</span>
<span id="cb14-50"><a href="#cb14-50" aria-hidden="true"></a>    plt.contourf(xx, yy, Z, alpha<span class="op">=</span><span class="fl">0.7</span>, cmap<span class="op">=</span><span class="st">&#39;Blues&#39;</span>)</span>
<span id="cb14-51"><a href="#cb14-51" aria-hidden="true"></a></span>
<span id="cb14-52"><a href="#cb14-52" aria-hidden="true"></a>    <span class="co"># Plot data points</span></span>
<span id="cb14-53"><a href="#cb14-53" aria-hidden="true"></a>    markers <span class="op">=</span> [<span class="st">&#39;o&#39;</span>, <span class="st">&#39;^&#39;</span>, <span class="st">&#39;s&#39;</span>]</span>
<span id="cb14-54"><a href="#cb14-54" aria-hidden="true"></a>    <span class="cf">for</span> class_label, marker <span class="kw">in</span> <span class="bu">zip</span>(np.unique(y), markers):</span>
<span id="cb14-55"><a href="#cb14-55" aria-hidden="true"></a>        plt.scatter(</span>
<span id="cb14-56"><a href="#cb14-56" aria-hidden="true"></a>            X[y <span class="op">==</span> class_label, <span class="dv">0</span>],</span>
<span id="cb14-57"><a href="#cb14-57" aria-hidden="true"></a>            X[y <span class="op">==</span> class_label, <span class="dv">1</span>],</span>
<span id="cb14-58"><a href="#cb14-58" aria-hidden="true"></a>            label<span class="op">=</span><span class="ss">f&quot;Class </span><span class="sc">{</span>class_label<span class="sc">}</span><span class="ss">&quot;</span> <span class="cf">if</span> i <span class="op">==</span> <span class="dv">0</span> <span class="cf">else</span> <span class="va">None</span>,</span>
<span id="cb14-59"><a href="#cb14-59" aria-hidden="true"></a>            marker<span class="op">=</span>marker,</span>
<span id="cb14-60"><a href="#cb14-60" aria-hidden="true"></a>            edgecolor<span class="op">=</span><span class="st">&quot;k&quot;</span>,</span>
<span id="cb14-61"><a href="#cb14-61" aria-hidden="true"></a>            s<span class="op">=</span><span class="dv">50</span></span>
<span id="cb14-62"><a href="#cb14-62" aria-hidden="true"></a>        )</span>
<span id="cb14-63"><a href="#cb14-63" aria-hidden="true"></a>    plt.title(<span class="ss">f&quot;C=</span><span class="sc">{C}</span><span class="ss">, Gamma=</span><span class="sc">{</span>gamma<span class="sc">}</span><span class="ch">\n</span><span class="ss">Train Acc: </span><span class="sc">{</span>train_acc<span class="sc">:.2f}</span><span class="ss">, Test Acc: </span><span class="sc">{</span>test_acc<span class="sc">:.2f}</span><span class="ss">&quot;</span>)</span>
<span id="cb14-64"><a href="#cb14-64" aria-hidden="true"></a>    plt.xlabel(<span class="st">&quot;Feature 1&quot;</span>)</span>
<span id="cb14-65"><a href="#cb14-65" aria-hidden="true"></a>    plt.ylabel(<span class="st">&quot;Feature 2&quot;</span>)</span>
<span id="cb14-66"><a href="#cb14-66" aria-hidden="true"></a></span>
<span id="cb14-67"><a href="#cb14-67" aria-hidden="true"></a><span class="co"># Adjust layout and add legend</span></span>
<span id="cb14-68"><a href="#cb14-68" aria-hidden="true"></a>plt.tight_layout()</span>
<span id="cb14-69"><a href="#cb14-69" aria-hidden="true"></a>plt.legend(loc<span class="op">=</span><span class="st">&#39;upper center&#39;</span>, bbox_to_anchor<span class="op">=</span>(<span class="op">-</span><span class="fl">0.1</span>, <span class="op">-</span><span class="fl">0.2</span>), title<span class="op">=</span><span class="st">&quot;Classes&quot;</span>, ncol<span class="op">=</span><span class="dv">3</span>)</span>
<span id="cb14-70"><a href="#cb14-70" aria-hidden="true"></a>plt.show()</span>
<span id="cb14-71"><a href="#cb14-71" aria-hidden="true"></a></span>
<span id="cb14-72"><a href="#cb14-72" aria-hidden="true"></a><span class="co"># Visualize accuracy as a function of C and Gamma</span></span>
<span id="cb14-73"><a href="#cb14-73" aria-hidden="true"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">6</span>))</span>
<span id="cb14-74"><a href="#cb14-74" aria-hidden="true"></a>plt.plot(C_values, results[<span class="st">&quot;train&quot;</span>], marker<span class="op">=</span><span class="st">&#39;o&#39;</span>, label<span class="op">=</span><span class="st">&quot;Train Accuracy&quot;</span>, color<span class="op">=</span><span class="st">&#39;blue&#39;</span>)</span>
<span id="cb14-75"><a href="#cb14-75" aria-hidden="true"></a>plt.plot(C_values, results[<span class="st">&quot;test&quot;</span>], marker<span class="op">=</span><span class="st">&#39;s&#39;</span>, label<span class="op">=</span><span class="st">&quot;Test Accuracy&quot;</span>, color<span class="op">=</span><span class="st">&#39;green&#39;</span>)</span>
<span id="cb14-76"><a href="#cb14-76" aria-hidden="true"></a>plt.xscale(<span class="st">&quot;log&quot;</span>)  <span class="co"># Log scale for better visualization</span></span>
<span id="cb14-77"><a href="#cb14-77" aria-hidden="true"></a>plt.xlabel(<span class="st">&quot;C (Regularization Parameter)&quot;</span>)</span>
<span id="cb14-78"><a href="#cb14-78" aria-hidden="true"></a>plt.ylabel(<span class="st">&quot;Accuracy&quot;</span>)</span>
<span id="cb14-79"><a href="#cb14-79" aria-hidden="true"></a>plt.title(<span class="st">&quot;Accuracy vs. C with RBF Kernel (Iris Dataset)&quot;</span>)</span>
<span id="cb14-80"><a href="#cb14-80" aria-hidden="true"></a>plt.legend()</span>
<span id="cb14-81"><a href="#cb14-81" aria-hidden="true"></a>plt.grid(<span class="va">True</span>)</span>
<span id="cb14-82"><a href="#cb14-82" aria-hidden="true"></a>plt.show()</span>
<span id="cb14-83"><a href="#cb14-83" aria-hidden="true"></a></span>
<span id="cb14-84"><a href="#cb14-84" aria-hidden="true"></a></span>
<span id="cb14-85"><a href="#cb14-85" aria-hidden="true"></a></span>
<span id="cb14-86"><a href="#cb14-86" aria-hidden="true"></a></span></code></pre></div>
<div class="output stream stderr">
<pre><code>WARNING:matplotlib.legend:No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
</code></pre>
</div>
<div class="output display_data">
<p><img src="0184428dfb0a9358a99c604eca2bed360de1ad08.png" /></p>
</div>
<div class="output display_data">
<p><img src="48298cc2699b05a5097e1db8a6cd18d0a8b664b1.png" /></p>
</div>
</div>
<section id="effects-of-gamma-and-c-on-model-behavior" class="cell markdown" id="_Oo-hSorTf8D">
<h4><strong>Effects of Gamma and C on Model Behavior</strong></h4>
<table>
<colgroup>
<col style="width: 13%" />
<col style="width: 41%" />
<col style="width: 44%" />
</colgroup>
<thead>
<tr class="header">
<th><strong>Parameter</strong></th>
<th><strong>Low Value</strong></th>
<th><strong>High Value</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Gamma (( \gamma ))</strong></td>
<td>- Smooth, generalized decision boundary.</td>
<td>- Complex, tight decision boundary.</td>
</tr>
<tr class="even">
<td></td>
<td>- Points far from the hyperplane have significant influence.</td>
<td>- Points close to the hyperplane dominate.</td>
</tr>
<tr class="odd">
<td></td>
<td>- May lead to <strong>underfitting</strong>, especially for non-linear data.</td>
<td>- Risk of <strong>overfitting</strong> due to excessive focus on local points.</td>
</tr>
<tr class="even">
<td></td>
<td>- Useful for broad patterns or low-dimensional data.</td>
<td>- Suitable for highly complex or non-linear data.</td>
</tr>
<tr class="odd">
<td><strong>C (Regularization)</strong></td>
<td>- Allows a larger margin with some misclassifications.</td>
<td>- Prioritizes minimizing misclassifications over margin size.</td>
</tr>
<tr class="even">
<td></td>
<td>- Encourages <strong>generalization</strong> (avoids overfitting).</td>
<td>- Can lead to <strong>overfitting</strong> by creating a complex decision boundary.</td>
</tr>
<tr class="odd">
<td></td>
<td>- Useful when avoiding overfitting is critical.</td>
<td>- Suitable for cases where misclassification penalties are high.</td>
</tr>
</tbody>
</table>
</section>
<section id="performance-comparison-of-svm-kernels" class="cell markdown" id="-PeZWX6EjK38">
<h3>Performance Comparison of SVM Kernels</h3>
<p>Below is a summary table comparing the performance metrics of Linear, Polynomial, and RBF kernels. These metrics include:</p>
<ul>
<li><strong>Accuracy</strong>: The percentage of correct predictions.</li>
<li><strong>Precision</strong>: The proportion of true positive predictions among all positive predictions.</li>
<li><strong>Recall</strong>: The proportion of true positive predictions among all actual positives.</li>
<li><strong>F1-Score</strong>: The harmonic mean of precision and recall.</li>
</ul>
<p>The table provides insights into how each kernel performs under the given conditions:</p>
</section>
<section id="results-and-observations" class="cell markdown" id="wvxEs7ZIfrlS">
<h2><strong>RESULTS AND OBSERVATIONS</strong></h2>
<p>Here, we present the key takeaways from the tutorial, including the performance of different kernels and their suitability for various types of data. A summary table with metrics such as accuracy, precision, recall, and F1-score is provided.</p>
<p>Key observations from the analysis are summarized below:</p>
<table>
<thead>
<tr class="header">
<th><strong>Kernel</strong></th>
<th><strong>Accuracy</strong></th>
<th><strong>Precision</strong></th>
<th><strong>Recall</strong></th>
<th><strong>F1-Score</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Linear</strong></td>
<td>97%</td>
<td>96%</td>
<td>97%</td>
<td>96%</td>
</tr>
<tr class="even">
<td><strong>Polynomial</strong></td>
<td>98%</td>
<td>97%</td>
<td>98%</td>
<td>98%</td>
</tr>
<tr class="odd">
<td><strong>RBF</strong></td>
<td>99%</td>
<td>98%</td>
<td>99%</td>
<td>99%</td>
</tr>
</tbody>
</table>
</section>
<div class="cell markdown" id="xkmO2Ta420Nd">
<p>In this section we can discuss when to use different kernels in real-world applications.</p>
<h3 id="linear-kernel"><strong>Linear Kernel</strong></h3>
<ul>
<li>Best for text classification (e.g., spam detection) or any linearly separable data.</li>
<li>Efficient for high-dimensional datasets.</li>
</ul>
<h3 id="polynomial-kernel"><strong>Polynomial Kernel</strong></h3>
<ul>
<li>Useful for image classification tasks where relationships are polynomial.</li>
<li>Can model curved boundaries in moderately complex datasets.</li>
</ul>
<h3 id="rbf-kernel"><strong>RBF Kernel</strong></h3>
<ul>
<li>Ideal for non-linear problems like handwriting recognition, medical diagnostics, and face detection.</li>
<li>Versatile but computationally expensive for large datasets.</li>
</ul>
<p>Choosing the right kernel depends on the dataset's complexity and computational resources.</p>
<h4 id="when-to-choose-each-kernel"><strong>When to Choose Each Kernel</strong></h4>
<p>Use the Linear kernel if your data is straightforward and linearly separable—it’s efficient and interpretable. Try the Polynomial kernel if you suspect polynomial patterns in your data and need more flexibility than the Linear kernel can offer. Choose the RBF kernel for datasets with complex, non-linear relationships or when other kernels fail to deliver good results. Key Considerations When selecting a kernel, there’s always a trade-off between flexibility and simplicity. Linear and Polynomial kernels are computationally efficient and interpretable, but they may struggle with non-linear data. On the other hand, the RBF kernel can handle complex patterns but requires careful tuning and is less interpretable.</p>
<p>By understanding your dataset’s structure and taking the time to tune hyperparameters, you can make an informed decision and unlock the full potential of SVMs for your classification tasks.</p>
</div>
<section id="conclusion" class="cell markdown" id="CH0jm52ZrbuZ">
<h2><strong>CONCLUSION</strong></h2>
<p>Support Vector Machines (SVMs) are a powerful and versatile tool for classification tasks, capable of handling both linear and non-linear data through the use of kernel functions. By comparing the Linear, Polynomial, and RBF kernels, we can draw several key insights into their strengths and limitations.</p>
<p><strong>The Linear kernel performed</strong> exceptionally well in this tutorial, achieving perfect accuracy. This result demonstrates that when the data is linearly separable, the Linear kernel is not only the most computationally efficient choice but also highly effective. Its simplicity makes it a strong candidate for datasets where linear decision boundaries suffice.</p>
<p><strong>The Polynomial kernel</strong>, configured with a degree of 3, also achieved perfect accuracy. This indicates that while it offers greater flexibility for capturing non-linear relationships, the dataset’s linear nature did not demand this additional complexity. However, its ability to adapt to more complex patterns makes it a valuable option for datasets that exhibit polynomial-like separability.</p>
<p><strong>The RBF kernel</strong> excelled at modeling non-linear decision boundaries. While its accuracy was slightly lower than that of the Linear and Polynomial kernels (97%), this kernel’s flexibility is unmatched for datasets with intricate, non-linear patterns. The choice of gamma significantly impacts the behavior of the RBF kernel: smaller values generalize the boundary, while larger values can lead to overfitting. Selecting an appropriate gamma is crucial to balancing flexibility and generalization.</p>
<hr />
<h4 id="recommendations"><strong>Recommendations</strong>:</h4>
<ul>
<li>Use <strong>Linear Kernel</strong> for simple datasets or high-dimensional sparse data (e.g., text classification).</li>
<li>Use <strong>Polynomial Kernel</strong> for moderately complex datasets where relationships are polynomial in nature.</li>
<li>Use <strong>RBF Kernel</strong> for non-linear datasets, especially when interpretability is less important than accuracy.</li>
</ul>
</section>
<section id="its-quiz-time" class="cell markdown" id="fTGPfbNclarE">
<h3><strong>IT'S QUIZ TIME</strong></h3>
<p>Answer the following questions to test your understanding of SVM kernels and their behavior. Select the correct answer for each question. You can use the explanations and visualizations from the tutorial to help you!</p>
</section>
<div class="cell code" data-execution_count="7" data-cellView="form" data-colab="{&quot;height&quot;:785,&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;,&quot;referenced_widgets&quot;:[&quot;b9deee951c01415ab5fe147277d5d2cd&quot;,&quot;83dd20783ec7470e9eaaf2c9ff2f2ebd&quot;,&quot;4fef29385fb24c7f84d29f6990a182df&quot;,&quot;6fa69120c62a459f93c1795616e5609b&quot;,&quot;60b9a2c91b4849dbace59d5575d82b5b&quot;,&quot;9ae7919593de4563a90b3e6de0d8cd03&quot;,&quot;67574055d66e46cf993438f51dbed3fa&quot;,&quot;40fbe04ef5d843058df7f7d3fed9a823&quot;,&quot;cd31a36596c94e23b40e3a36b79de63f&quot;,&quot;ba31e84b3b504abc9548dda5aa5d519f&quot;,&quot;8d963e1bdc6148ec9244dc9b975c9816&quot;,&quot;a2e05932cd85466d86a16dd8b6e66958&quot;,&quot;087dd86aeed747b193a082e15ed920c7&quot;,&quot;f8b593861d42401fa1bb43b68749c2be&quot;,&quot;c83da36c291243da914a03c1b43f543f&quot;,&quot;0c3c8fe3bc164f35b493f00bca387a2d&quot;,&quot;c9c20b4c767b4b43ab7267aade7029bc&quot;,&quot;426beb5a892c4aeabdb44bbcdf593b62&quot;,&quot;bbd6c469097a4996b25964593b1b21a4&quot;,&quot;4ee0333e8d0e4c2694e728158dc86f23&quot;,&quot;c3b7afc5d4c84b85827f55e81b1df9ed&quot;,&quot;45ffa4754ad64bfba29c71f68cff3bb5&quot;,&quot;a017340b5fbb42cd8d1ee506c6c6f162&quot;,&quot;aceabd595cf6478c9c1bb1873da4d670&quot;,&quot;e6a0736a697a4b5ca2ba5e95d2d2f786&quot;,&quot;b294082f5fb2427cbd650e7c28dcea2d&quot;,&quot;7e4359338965457eb70bd3a8aa092af1&quot;,&quot;29ece8cefd24405cbb182befe5bdf9dc&quot;,&quot;c423f4d569f840eea57877d80d7b75ec&quot;,&quot;01944f70db3d40f7bd447ffd8a2edb2d&quot;,&quot;a0a06241417b4a2abe4ba0c7cdfd183e&quot;,&quot;aad00e0976c14ef4a269928e8172aec5&quot;,&quot;86b8a5254d5d4edeaa0496a40ff4f14a&quot;,&quot;e8a8f4b987ad4f4b938068e9646520dc&quot;,&quot;556a7473e6a241b3aa4df15323a8adfb&quot;,&quot;0438f2de52154d68851eb7c5231ca52a&quot;,&quot;fff579e818cf492093b747d14c67537e&quot;,&quot;dd19b06ff9f243aea7c5466e57d6d2fe&quot;,&quot;c913bd18d591487a91c18d7a256acf58&quot;,&quot;fb48be6d00fc41a8a9db0fefd10a3c29&quot;,&quot;0291801717254d4a90eb0a27ab4f0bbf&quot;,&quot;4a383be31cc54809979a598e121ec13d&quot;,&quot;97e09b35a2834563b7195a3eadeeefbb&quot;,&quot;7aa7038696f648eeb8913de21fe2156b&quot;,&quot;bd76cd7072f24dcda314d95153f44525&quot;,&quot;740c16494dc34b0ea8d10152581f474a&quot;,&quot;1be9c12343eb42a1b2073c22e7f74a2b&quot;,&quot;ece71d14c4854c1d8e52afd65dc743af&quot;,&quot;16c2989cdc8842aab880738c3fe4a2c5&quot;,&quot;716f4b03ae014eeb9fa17808c0a11bc6&quot;,&quot;8b3ddbea7ecc420c91c2b6c4cd426b65&quot;,&quot;a76eaee6c04b470ca54cb568a17e24ca&quot;,&quot;0aa65a9c58a5456194344f6eb927064a&quot;,&quot;1a45e081a7d242ec86fc2d723ca89d54&quot;,&quot;454fac0269f242999b9e4dece5fc8acf&quot;,&quot;e3c5cbc73591416d8ee0ce656aa4db0a&quot;,&quot;4973128f8f32401ca2b3ab6418b6f821&quot;,&quot;698c381198fd4d23896fd78c436da700&quot;,&quot;3252e440cf2746bdb85ca228cae749d9&quot;,&quot;3cea01e96cf747c4a91113b7651195b0&quot;,&quot;d69460431a8945f39b5a4565b41a5d34&quot;,&quot;587c5aa32e3e4408980d6e5c5f29ee21&quot;,&quot;91e0feb4a9d840eabc21c4b61441979d&quot;,&quot;473d7c6b499647cd82883bc103372e2b&quot;,&quot;d5df25f652544ce6b29909242b4f0ab0&quot;,&quot;95d05f8a954143aebb3b4674686f89fb&quot;,&quot;5c6653ad21c849808752f6510bd664ce&quot;,&quot;56c34b210ad9440b9abd52e8b0d79ac6&quot;,&quot;ba95f90c5eb14ef98cd5011833b139a3&quot;,&quot;909d9ef1a7f0413c9ec20c815c133d5a&quot;,&quot;528047ee332c4c3f8d15c82337aa194e&quot;,&quot;5a82fdf5e119470db896aa001119d490&quot;,&quot;fbe67aa0eb60400088e3d8a6563c568b&quot;]}" id="qrQRIbGdlQi1" data-outputId="517d583f-4ee5-4376-bb7e-709647a0bdf0">
<div class="sourceCode" id="cb16"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true"></a><span class="co"># @title</span></span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true"></a><span class="im">from</span> ipywidgets <span class="im">import</span> widgets, VBox, Label, Button, Output, HTML</span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true"></a></span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true"></a><span class="co"># Define questions with the updated first question</span></span>
<span id="cb16-5"><a href="#cb16-5" aria-hidden="true"></a>questions <span class="op">=</span> [</span>
<span id="cb16-6"><a href="#cb16-6" aria-hidden="true"></a>    {</span>
<span id="cb16-7"><a href="#cb16-7" aria-hidden="true"></a>        <span class="st">&quot;question&quot;</span>: <span class="st">&quot;What is the primary objective of SVM?&quot;</span>,</span>
<span id="cb16-8"><a href="#cb16-8" aria-hidden="true"></a>        <span class="st">&quot;options&quot;</span>: [</span>
<span id="cb16-9"><a href="#cb16-9" aria-hidden="true"></a>            <span class="st">&quot;To maximize the number of support vectors&quot;</span>,</span>
<span id="cb16-10"><a href="#cb16-10" aria-hidden="true"></a>            <span class="st">&quot;To maximize the margin between classes&quot;</span>,</span>
<span id="cb16-11"><a href="#cb16-11" aria-hidden="true"></a>            <span class="st">&quot;To minimize the training error&quot;</span>,</span>
<span id="cb16-12"><a href="#cb16-12" aria-hidden="true"></a>            <span class="st">&quot;To select the best kernel automatically&quot;</span></span>
<span id="cb16-13"><a href="#cb16-13" aria-hidden="true"></a>        ],</span>
<span id="cb16-14"><a href="#cb16-14" aria-hidden="true"></a>        <span class="st">&quot;answer&quot;</span>: <span class="st">&quot;To maximize the margin between classes&quot;</span></span>
<span id="cb16-15"><a href="#cb16-15" aria-hidden="true"></a>    },</span>
<span id="cb16-16"><a href="#cb16-16" aria-hidden="true"></a>    {</span>
<span id="cb16-17"><a href="#cb16-17" aria-hidden="true"></a>        <span class="st">&quot;question&quot;</span>: <span class="st">&quot;Which kernel is best for linearly separable data?&quot;</span>,</span>
<span id="cb16-18"><a href="#cb16-18" aria-hidden="true"></a>        <span class="st">&quot;options&quot;</span>: [<span class="st">&quot;Linear&quot;</span>, <span class="st">&quot;Polynomial&quot;</span>, <span class="st">&quot;RBF&quot;</span>, <span class="st">&quot;Sigmoid&quot;</span>],</span>
<span id="cb16-19"><a href="#cb16-19" aria-hidden="true"></a>        <span class="st">&quot;answer&quot;</span>: <span class="st">&quot;Linear&quot;</span></span>
<span id="cb16-20"><a href="#cb16-20" aria-hidden="true"></a>    },</span>
<span id="cb16-21"><a href="#cb16-21" aria-hidden="true"></a>    {</span>
<span id="cb16-22"><a href="#cb16-22" aria-hidden="true"></a>        <span class="st">&quot;question&quot;</span>: <span class="st">&quot;What does the C parameter control in SVM?&quot;</span>,</span>
<span id="cb16-23"><a href="#cb16-23" aria-hidden="true"></a>        <span class="st">&quot;options&quot;</span>: [</span>
<span id="cb16-24"><a href="#cb16-24" aria-hidden="true"></a>            <span class="st">&quot;Kernel type&quot;</span>,</span>
<span id="cb16-25"><a href="#cb16-25" aria-hidden="true"></a>            <span class="st">&quot;Margin width&quot;</span>,</span>
<span id="cb16-26"><a href="#cb16-26" aria-hidden="true"></a>            <span class="st">&quot;Data scaling&quot;</span>,</span>
<span id="cb16-27"><a href="#cb16-27" aria-hidden="true"></a>            <span class="st">&quot;Number of support vectors&quot;</span></span>
<span id="cb16-28"><a href="#cb16-28" aria-hidden="true"></a>        ],</span>
<span id="cb16-29"><a href="#cb16-29" aria-hidden="true"></a>        <span class="st">&quot;answer&quot;</span>: <span class="st">&quot;Margin width&quot;</span></span>
<span id="cb16-30"><a href="#cb16-30" aria-hidden="true"></a>    },</span>
<span id="cb16-31"><a href="#cb16-31" aria-hidden="true"></a>    {</span>
<span id="cb16-32"><a href="#cb16-32" aria-hidden="true"></a>        <span class="st">&quot;question&quot;</span>: <span class="st">&quot;Which kernel is most flexible for non-linear data?&quot;</span>,</span>
<span id="cb16-33"><a href="#cb16-33" aria-hidden="true"></a>        <span class="st">&quot;options&quot;</span>: [<span class="st">&quot;Linear&quot;</span>, <span class="st">&quot;Polynomial&quot;</span>, <span class="st">&quot;RBF&quot;</span>, <span class="st">&quot;Sigmoid&quot;</span>],</span>
<span id="cb16-34"><a href="#cb16-34" aria-hidden="true"></a>        <span class="st">&quot;answer&quot;</span>: <span class="st">&quot;RBF&quot;</span></span>
<span id="cb16-35"><a href="#cb16-35" aria-hidden="true"></a>    }</span>
<span id="cb16-36"><a href="#cb16-36" aria-hidden="true"></a>]</span>
<span id="cb16-37"><a href="#cb16-37" aria-hidden="true"></a></span>
<span id="cb16-38"><a href="#cb16-38" aria-hidden="true"></a>score <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb16-39"><a href="#cb16-39" aria-hidden="true"></a>feedback_output <span class="op">=</span> Output()</span>
<span id="cb16-40"><a href="#cb16-40" aria-hidden="true"></a></span>
<span id="cb16-41"><a href="#cb16-41" aria-hidden="true"></a><span class="kw">def</span> create_quiz(questions):</span>
<span id="cb16-42"><a href="#cb16-42" aria-hidden="true"></a>    <span class="kw">global</span> score</span>
<span id="cb16-43"><a href="#cb16-43" aria-hidden="true"></a>    score <span class="op">=</span> <span class="dv">0</span>  <span class="co"># Reset score</span></span>
<span id="cb16-44"><a href="#cb16-44" aria-hidden="true"></a>    quiz_elements <span class="op">=</span> []</span>
<span id="cb16-45"><a href="#cb16-45" aria-hidden="true"></a></span>
<span id="cb16-46"><a href="#cb16-46" aria-hidden="true"></a>    <span class="kw">def</span> evaluate_answer_factory(question_idx):</span>
<span id="cb16-47"><a href="#cb16-47" aria-hidden="true"></a>        <span class="kw">def</span> evaluate_answer(button):</span>
<span id="cb16-48"><a href="#cb16-48" aria-hidden="true"></a>            <span class="kw">global</span> score</span>
<span id="cb16-49"><a href="#cb16-49" aria-hidden="true"></a>            <span class="cf">with</span> feedback_output:</span>
<span id="cb16-50"><a href="#cb16-50" aria-hidden="true"></a>                feedback_output.clear_output()</span>
<span id="cb16-51"><a href="#cb16-51" aria-hidden="true"></a>                user_answer <span class="op">=</span> radio_buttons[question_idx].value</span>
<span id="cb16-52"><a href="#cb16-52" aria-hidden="true"></a>                <span class="cf">if</span> user_answer <span class="op">==</span> questions[question_idx][<span class="st">&quot;answer&quot;</span>]:</span>
<span id="cb16-53"><a href="#cb16-53" aria-hidden="true"></a>                    <span class="bu">print</span>(<span class="ss">f&quot;✅ Correct! The answer is &#39;</span><span class="sc">{</span>user_answer<span class="sc">}</span><span class="ss">&#39;.&quot;</span>)</span>
<span id="cb16-54"><a href="#cb16-54" aria-hidden="true"></a>                    score <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb16-55"><a href="#cb16-55" aria-hidden="true"></a>                <span class="cf">else</span>:</span>
<span id="cb16-56"><a href="#cb16-56" aria-hidden="true"></a>                    <span class="bu">print</span>(<span class="ss">f&quot;❌ Incorrect. The correct answer is &#39;</span><span class="sc">{</span>questions[question_idx][<span class="st">&#39;answer&#39;</span>]<span class="sc">}</span><span class="ss">&#39;.&quot;</span>)</span>
<span id="cb16-57"><a href="#cb16-57" aria-hidden="true"></a>        <span class="cf">return</span> evaluate_answer</span>
<span id="cb16-58"><a href="#cb16-58" aria-hidden="true"></a></span>
<span id="cb16-59"><a href="#cb16-59" aria-hidden="true"></a>    radio_buttons <span class="op">=</span> []</span>
<span id="cb16-60"><a href="#cb16-60" aria-hidden="true"></a>    <span class="cf">for</span> idx, q <span class="kw">in</span> <span class="bu">enumerate</span>(questions):</span>
<span id="cb16-61"><a href="#cb16-61" aria-hidden="true"></a>        question_label <span class="op">=</span> Label(<span class="ss">f&quot;Q</span><span class="sc">{</span>idx <span class="op">+</span> <span class="dv">1</span><span class="sc">}</span><span class="ss">: </span><span class="sc">{q</span>[<span class="st">&#39;question&#39;</span>]<span class="sc">}</span><span class="ss">&quot;</span>)</span>
<span id="cb16-62"><a href="#cb16-62" aria-hidden="true"></a>        radio_buttons.append(widgets.RadioButtons(</span>
<span id="cb16-63"><a href="#cb16-63" aria-hidden="true"></a>            options<span class="op">=</span>q[<span class="st">&quot;options&quot;</span>],</span>
<span id="cb16-64"><a href="#cb16-64" aria-hidden="true"></a>            description<span class="op">=</span><span class="st">&quot;&quot;</span>,</span>
<span id="cb16-65"><a href="#cb16-65" aria-hidden="true"></a>            style<span class="op">=</span>{<span class="st">&quot;description_width&quot;</span>: <span class="st">&quot;initial&quot;</span>}</span>
<span id="cb16-66"><a href="#cb16-66" aria-hidden="true"></a>        ))</span>
<span id="cb16-67"><a href="#cb16-67" aria-hidden="true"></a>        submit_button <span class="op">=</span> Button(description<span class="op">=</span><span class="ss">f&quot;Submit Answer for Question </span><span class="sc">{</span>idx <span class="op">+</span> <span class="dv">1</span><span class="sc">}</span><span class="ss">&quot;</span>)</span>
<span id="cb16-68"><a href="#cb16-68" aria-hidden="true"></a>        submit_button.on_click(evaluate_answer_factory(idx))</span>
<span id="cb16-69"><a href="#cb16-69" aria-hidden="true"></a>        quiz_elements.append(VBox([</span>
<span id="cb16-70"><a href="#cb16-70" aria-hidden="true"></a>            question_label,</span>
<span id="cb16-71"><a href="#cb16-71" aria-hidden="true"></a>            radio_buttons[idx],</span>
<span id="cb16-72"><a href="#cb16-72" aria-hidden="true"></a>            submit_button,</span>
<span id="cb16-73"><a href="#cb16-73" aria-hidden="true"></a>            widgets.HTML(value<span class="op">=</span><span class="st">&quot;&lt;hr&gt;&quot;</span>)  <span class="co"># Add separator after each question</span></span>
<span id="cb16-74"><a href="#cb16-74" aria-hidden="true"></a>        ]))</span>
<span id="cb16-75"><a href="#cb16-75" aria-hidden="true"></a></span>
<span id="cb16-76"><a href="#cb16-76" aria-hidden="true"></a>    <span class="cf">return</span> VBox(quiz_elements)</span>
<span id="cb16-77"><a href="#cb16-77" aria-hidden="true"></a></span>
<span id="cb16-78"><a href="#cb16-78" aria-hidden="true"></a><span class="co"># Create and display the quiz</span></span>
<span id="cb16-79"><a href="#cb16-79" aria-hidden="true"></a>quiz_interface <span class="op">=</span> create_quiz(questions)</span>
<span id="cb16-80"><a href="#cb16-80" aria-hidden="true"></a>display(VBox([HTML(value<span class="op">=</span><span class="st">&quot;&lt;h2&gt;SVM Quiz: Test Your Knowledge&lt;/h2&gt;&quot;</span>), quiz_interface, feedback_output]))</span>
<span id="cb16-81"><a href="#cb16-81" aria-hidden="true"></a></span>
<span id="cb16-82"><a href="#cb16-82" aria-hidden="true"></a><span class="co"># Display the final score dynamically</span></span>
<span id="cb16-83"><a href="#cb16-83" aria-hidden="true"></a><span class="kw">def</span> show_score(button):</span>
<span id="cb16-84"><a href="#cb16-84" aria-hidden="true"></a>    <span class="cf">with</span> feedback_output:</span>
<span id="cb16-85"><a href="#cb16-85" aria-hidden="true"></a>        feedback_output.clear_output()</span>
<span id="cb16-86"><a href="#cb16-86" aria-hidden="true"></a>        <span class="bu">print</span>(<span class="ss">f&quot;Your final score: </span><span class="sc">{</span>score<span class="sc">}</span><span class="ss">/</span><span class="sc">{</span><span class="bu">len</span>(questions)<span class="sc">}</span><span class="ss">&quot;</span>)</span>
<span id="cb16-87"><a href="#cb16-87" aria-hidden="true"></a>        <span class="cf">if</span> score <span class="op">==</span> <span class="bu">len</span>(questions):</span>
<span id="cb16-88"><a href="#cb16-88" aria-hidden="true"></a>            <span class="bu">print</span>(<span class="st">&quot;🎉 Excellent! You got all answers correct!&quot;</span>)</span>
<span id="cb16-89"><a href="#cb16-89" aria-hidden="true"></a>        <span class="cf">elif</span> score <span class="op">&gt;</span> <span class="bu">len</span>(questions) <span class="op">//</span> <span class="dv">2</span>:</span>
<span id="cb16-90"><a href="#cb16-90" aria-hidden="true"></a>            <span class="bu">print</span>(<span class="st">&quot;😊 Good job! Keep practicing to perfect your understanding.&quot;</span>)</span>
<span id="cb16-91"><a href="#cb16-91" aria-hidden="true"></a>        <span class="cf">else</span>:</span>
<span id="cb16-92"><a href="#cb16-92" aria-hidden="true"></a>            <span class="bu">print</span>(<span class="st">&quot;😕 Don&#39;t worry, review the concepts and try again!&quot;</span>)</span>
<span id="cb16-93"><a href="#cb16-93" aria-hidden="true"></a></span>
<span id="cb16-94"><a href="#cb16-94" aria-hidden="true"></a>score_button <span class="op">=</span> Button(description<span class="op">=</span><span class="st">&quot;Show Final Score&quot;</span>, button_style<span class="op">=</span><span class="st">&#39;success&#39;</span>)</span>
<span id="cb16-95"><a href="#cb16-95" aria-hidden="true"></a>score_button.on_click(show_score)</span>
<span id="cb16-96"><a href="#cb16-96" aria-hidden="true"></a>display(VBox([HTML(value<span class="op">=</span><span class="st">&quot;&lt;hr&gt;&quot;</span>), score_button]))</span>
<span id="cb16-97"><a href="#cb16-97" aria-hidden="true"></a></span>
<span id="cb16-98"><a href="#cb16-98" aria-hidden="true"></a></span></code></pre></div>
<div class="output display_data">
<div class="sourceCode" id="cb17"><pre class="sourceCode json"><code class="sourceCode json"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true"></a><span class="fu">{</span><span class="dt">&quot;model_id&quot;</span><span class="fu">:</span><span class="st">&quot;b9deee951c01415ab5fe147277d5d2cd&quot;</span><span class="fu">,</span><span class="dt">&quot;version_major&quot;</span><span class="fu">:</span><span class="dv">2</span><span class="fu">,</span><span class="dt">&quot;version_minor&quot;</span><span class="fu">:</span><span class="dv">0</span><span class="fu">}</span></span></code></pre></div>
</div>
<div class="output display_data">
<div class="sourceCode" id="cb18"><pre class="sourceCode json"><code class="sourceCode json"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true"></a><span class="fu">{</span><span class="dt">&quot;model_id&quot;</span><span class="fu">:</span><span class="st">&quot;95d05f8a954143aebb3b4674686f89fb&quot;</span><span class="fu">,</span><span class="dt">&quot;version_major&quot;</span><span class="fu">:</span><span class="dv">2</span><span class="fu">,</span><span class="dt">&quot;version_minor&quot;</span><span class="fu">:</span><span class="dv">0</span><span class="fu">}</span></span></code></pre></div>
</div>
</div>
<section id="references" class="cell markdown" id="8NM4eNIcrhUl">
<h2><strong>References</strong></h2>
<ul>
<li><p>Scikit-learn Documentation: <a href="https://scikit-learn.org/stable/" class="uri">https://scikit-learn.org/stable/</a></p></li>
<li><p>SVM Kernels: <a href="https://scikit-learn.org/stable/modules/svm.html#" class="uri">https://scikit-learn.org/stable/modules/svm.html#</a></p></li>
<li><p>Research Paper: Cortes, C., &amp; Vapnik, V. (1995). Support-vector networks. Machine learning, 20(3), 273-297.(<a href="https://link.springer.com/article/10.1007/BF00994018" class="uri">https://link.springer.com/article/10.1007/BF00994018</a>) -Understanding Support Vector Machine(SVM) Algorithm from Examples: <a href="https://www.geeksforgeeks.org/support-vector-machine-algorithm/(Analytics" class="uri">https://www.geeksforgeeks.org/support-vector-machine-algorithm/(Analytics</a> Vidhya)</p></li>
<li><p>Wong, B. (2011). Color Universal Design (CUD): How to make figures and presentations that are friendly to colorblind people. <a href="https://jfly.uni-koeln.de/color/" class="uri">https://jfly.uni-koeln.de/color/</a></p></li>
<li><p>UCI Machine Learning Repository: Iris Dataset <a href="https://archive.ics.uci.edu/dataset/53/iris" class="uri">https://archive.ics.uci.edu/dataset/53/iris</a></p></li>
<li><p>Gamma Parameters in SVM: <a href="https://www.geeksforgeeks.org/gamma-parameter-in-svm/" class="uri">https://www.geeksforgeeks.org/gamma-parameter-in-svm/</a></p></li>
<li><p>Guidelines for creating visualizations that are accessible to colorblind individuals. <a href="https://jfly.uni-koeln.de/color/" class="uri">https://jfly.uni-koeln.de/color/</a></p></li>
<li><p>SVM Tutorial. "Understanding the Math behind Support Vector Machines – Part 1." November 2014. <a href="https://www.svm-tutorial.com/2014/11/svm-understanding-math-part-1/">https://www.svm-tutorial.com/2014/11/svm-understanding-math-part-1/</a>.</p></li>
</ul>
</section>
