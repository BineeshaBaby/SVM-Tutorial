<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01//EN"
   "http://www.w3.org/TR/html4/strict.dtd">
<!--
generated by Pygments <https://pygments.org/>
Copyright 2006-2024 by the Pygments team.
Licensed under the BSD license, see LICENSE for details.
-->
<html>
<head>
  <title></title>
  <meta http-equiv="content-type" content="text/html; charset=utf-8">
  <style type="text/css">
/*
generated by Pygments <https://pygments.org/>
Copyright 2006-2024 by the Pygments team.
Licensed under the BSD license, see LICENSE for details.
*/
pre { line-height: 125%; }
td.linenos .normal { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; }
span.linenos { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; }
td.linenos .special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }
span.linenos.special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }
body .hll { background-color: #ffffcc }
body { background: #f8f8f8; }
body .c { color: #3D7B7B; font-style: italic } /* Comment */
body .err { border: 1px solid #FF0000 } /* Error */
body .k { color: #008000; font-weight: bold } /* Keyword */
body .o { color: #666666 } /* Operator */
body .ch { color: #3D7B7B; font-style: italic } /* Comment.Hashbang */
body .cm { color: #3D7B7B; font-style: italic } /* Comment.Multiline */
body .cp { color: #9C6500 } /* Comment.Preproc */
body .cpf { color: #3D7B7B; font-style: italic } /* Comment.PreprocFile */
body .c1 { color: #3D7B7B; font-style: italic } /* Comment.Single */
body .cs { color: #3D7B7B; font-style: italic } /* Comment.Special */
body .gd { color: #A00000 } /* Generic.Deleted */
body .ge { font-style: italic } /* Generic.Emph */
body .ges { font-weight: bold; font-style: italic } /* Generic.EmphStrong */
body .gr { color: #E40000 } /* Generic.Error */
body .gh { color: #000080; font-weight: bold } /* Generic.Heading */
body .gi { color: #008400 } /* Generic.Inserted */
body .go { color: #717171 } /* Generic.Output */
body .gp { color: #000080; font-weight: bold } /* Generic.Prompt */
body .gs { font-weight: bold } /* Generic.Strong */
body .gu { color: #800080; font-weight: bold } /* Generic.Subheading */
body .gt { color: #0044DD } /* Generic.Traceback */
body .kc { color: #008000; font-weight: bold } /* Keyword.Constant */
body .kd { color: #008000; font-weight: bold } /* Keyword.Declaration */
body .kn { color: #008000; font-weight: bold } /* Keyword.Namespace */
body .kp { color: #008000 } /* Keyword.Pseudo */
body .kr { color: #008000; font-weight: bold } /* Keyword.Reserved */
body .kt { color: #B00040 } /* Keyword.Type */
body .m { color: #666666 } /* Literal.Number */
body .s { color: #BA2121 } /* Literal.String */
body .na { color: #687822 } /* Name.Attribute */
body .nb { color: #008000 } /* Name.Builtin */
body .nc { color: #0000FF; font-weight: bold } /* Name.Class */
body .no { color: #880000 } /* Name.Constant */
body .nd { color: #AA22FF } /* Name.Decorator */
body .ni { color: #717171; font-weight: bold } /* Name.Entity */
body .ne { color: #CB3F38; font-weight: bold } /* Name.Exception */
body .nf { color: #0000FF } /* Name.Function */
body .nl { color: #767600 } /* Name.Label */
body .nn { color: #0000FF; font-weight: bold } /* Name.Namespace */
body .nt { color: #008000; font-weight: bold } /* Name.Tag */
body .nv { color: #19177C } /* Name.Variable */
body .ow { color: #AA22FF; font-weight: bold } /* Operator.Word */
body .w { color: #bbbbbb } /* Text.Whitespace */
body .mb { color: #666666 } /* Literal.Number.Bin */
body .mf { color: #666666 } /* Literal.Number.Float */
body .mh { color: #666666 } /* Literal.Number.Hex */
body .mi { color: #666666 } /* Literal.Number.Integer */
body .mo { color: #666666 } /* Literal.Number.Oct */
body .sa { color: #BA2121 } /* Literal.String.Affix */
body .sb { color: #BA2121 } /* Literal.String.Backtick */
body .sc { color: #BA2121 } /* Literal.String.Char */
body .dl { color: #BA2121 } /* Literal.String.Delimiter */
body .sd { color: #BA2121; font-style: italic } /* Literal.String.Doc */
body .s2 { color: #BA2121 } /* Literal.String.Double */
body .se { color: #AA5D1F; font-weight: bold } /* Literal.String.Escape */
body .sh { color: #BA2121 } /* Literal.String.Heredoc */
body .si { color: #A45A77; font-weight: bold } /* Literal.String.Interpol */
body .sx { color: #008000 } /* Literal.String.Other */
body .sr { color: #A45A77 } /* Literal.String.Regex */
body .s1 { color: #BA2121 } /* Literal.String.Single */
body .ss { color: #19177C } /* Literal.String.Symbol */
body .bp { color: #008000 } /* Name.Builtin.Pseudo */
body .fm { color: #0000FF } /* Name.Function.Magic */
body .vc { color: #19177C } /* Name.Variable.Class */
body .vg { color: #19177C } /* Name.Variable.Global */
body .vi { color: #19177C } /* Name.Variable.Instance */
body .vm { color: #19177C } /* Name.Variable.Magic */
body .il { color: #666666 } /* Literal.Number.Integer.Long */

  </style>
</head>
<body>
<h2></h2>

<div class="highlight"><pre><span></span><span class="c1"># -*- coding: utf-8 -*-</span>
<span class="sd">&quot;&quot;&quot;SVM.ipynb</span>

<span class="sd">Automatically generated by Colab.</span>

<span class="sd">Original file is located at</span>
<span class="sd">    https://colab.research.google.com/drive/1jL5PqlokHCj8_6HLCcYF3ZTCdef_64O6</span>

<span class="sd">#     **Exploring Kernel Functions in Support Vector Machines (SVM)**</span>

<span class="sd">A Practical and Visual Guide to Understanding the Role of Kernels and Hyperparameter Tuning in SVMs</span>



<span class="sd">---</span>


<span class="sd">## **INTRODUCTION**</span>
<span class="sd">Support Vector Machines (SVMs) are powerful tools in the machine learning arsenal, designed to tackle both classification and regression tasks with precision and accuracy. What sets SVM apart is its ability to find the optimal boundary, or hyperplane, that separates different classes of data points. But what happens when the data isn&#39;t linearly separable? Enter kernels – the secret ingredient that transforms SVM into a versatile powerhouse capable of handling even the most complex datasets.</span>


<span class="sd">The goal of a support vector machine is to find the optimal separating hyperplane which maximizes the margin of the training data [SVM Tutorial, 2014](https://www.svm-tutorial.com/2014/11/svm-understanding-math-part-1/).</span>

<span class="sd">Kernels are mathematical functions that enable SVM to map data into higher-dimensional spaces, where non-linear patterns become linearly separable. This process, known as the kernel trick, allows SVM to achieve exceptional performance without the computational overhead of explicitly transforming the data.</span>

<span class="sd">In this guide, we’ll explore how different kernel functions work, how to tune their hyperparameters effectively, and how they shape decision boundaries. Using a mix of theoretical explanations, practical examples, and visualizations, you’ll gain a deep understanding of how to harness the full potential of SVMs for your own projects.</span>


<span class="sd">---</span>
<span class="sd">### **Why Kernels Matter**</span>
<span class="sd">Kernels are the key to unlocking SVM’s ability to handle non-linear relationships in data. By using the right kernel, you can transform an SVM from a simple classifier to a sophisticated tool capable of solving complex real-world problems.</span>

<span class="sd">From spam detection to medical diagnostics and image recognition, SVMs equipped with kernels have proven their worth across industries. Understanding how kernels work and learning to fine-tune them is essential for building accurate and efficient machine learning models.</span>

<span class="sd">By the end of this guide, you’ll not only know how to implement SVMs with kernels but also how to visualize their behavior, tune their parameters, and select the right kernel for the job.</span>

<span class="sd">### **What You&#39;ll Learn**</span>

<span class="sd">*   How Support Vector Machines classify data with the help of kernel functions.</span>

<span class="sd">*  The differences between linear, polynomial, and radial basis function (RBF) kernels, and when to use each one.</span>

<span class="sd">*  How hyperparameter tuning impacts decision boundaries and overall performance.</span>

<span class="sd">*  How to visualize SVM decision boundaries using interactive Python plots.</span>

<span class="sd">This tutorial bridges theory and practice, showing how to apply SVMs with different kernels in real-world scenarios.</span>




<span class="sd">---</span>



<span class="sd">### **Why It Matters**</span>
<span class="sd">SVMs are widely used in tasks that demand high accuracy and precision, such as:</span>
<span class="sd">*   Spam detection: Sorting unwanted emails effectively.</span>
<span class="sd">*   Medical diagnostics: Classifying diseases based on patient data.</span>

<span class="sd">*   Image recognition: Identifying objects, faces, or patterns in images.</span>

<span class="sd">By mastering kernels, you’ll gain the ability to tailor SVMs to different types of data, unlocking their full potential. This skill is not just theoretical – it’s a practical, job-ready capability for tackling real-world challenges.</span>

<span class="sd">Let’s dive in and explore how kernels transform SVMs into versatile, high-performance tools for machine learning.</span>

<span class="sd">## **OBJECTIVES**</span>
<span class="sd">In this section, we outline the key goals of the tutorial.</span>

<span class="sd">1. Explain the theory behind SVM and kernel functions.</span>
<span class="sd">2. Show how hyperparameter tuning  impacts decision boundaries and performance.</span>
<span class="sd">3. Compare kernel effectiveness using visualizations and performance metrics.</span>

<span class="sd">## **THEORETICAL BACKGROUND**</span>
<span class="sd">#### What are SVMs?</span>
<span class="sd">A supervised learning algorithm that finds the optimal hyperplane to separate data points into classes. Support Vector Machines work by finding a hyperplane that separates data points into different classes.</span>
<span class="sd">When data is not linearly separable, **kernel functions** are used to map the data into a higher-dimensional space where a linear separator can exist.</span>
<span class="sd">### Common Kernels</span>


<span class="sd">*   **Linear Kernel**:  Computes a simple dot product between data points in the input space.</span>

<span class="sd">   - \( K(x_i, x_j) = x_i \cdot x_j \)</span>

<span class="sd">   Great for datasets where the data is already linearly separable. It’s often used for text classification problems, like spam detection.</span>
<span class="sd">*   **Polynomial Kernel**: Captures polynomial relationships between data points.</span>
<span class="sd">   - \( K(x_i, x_j) = (x_i \cdot x_j + c)^d \)</span>
<span class="sd">   Captures complex, curved relationships in data. It’s useful for moderately intricate tasks like certain image classifications.</span>
<span class="sd">*   **Radial Basis Function (RBF) Kernel**: Maps data points into an infinite-dimensional space.</span>
<span class="sd">   - \( K(x_i, x_j) = \exp(-\gamma \|x_i - x_j\|^2) \)</span>

<span class="sd">The most flexible option, perfect for data with non-linear patterns, like recognizing objects in images or patterns in biological data.</span>

<span class="sd">## **PRACTICAL IMPLEMENTATION**</span>

<span class="sd">### Dataset Preparation</span>
<span class="sd">We use two datasets for this tutorial:</span>

<span class="sd">### Iris Dataset</span>

<span class="sd">The Iris dataset is a well-known dataset comprising three flower species(Setosa, Versicolor, Virginica) with four features each. It includes both linearly separable and non-linear class distributions.</span>
<span class="sd"> There are **150 samples** in total, with each sample described by **four features**:</span>

<span class="sd">*   Sepal Length - Length of the sepal (cm)</span>
<span class="sd">*   Sepal Width  - Width of the sepal (cm)</span>
<span class="sd">*   Petal Length - Length of the petal(cm)</span>
<span class="sd">*   Petal Width  - Width of the petal (cm)</span>



<span class="sd">This dataset is perfect for demonstrating SVM kernels because it includes both linearly separable and non-linear class distributions.</span>


<span class="sd">---</span>


<span class="sd">### Synthetic Dataset</span>
<span class="sd">To make things even clearer, we’ll use synthetic data generated with two distinct clusters. This helps us visualize how SVM kernels draw boundaries between classes.</span>

<span class="sd">### **Step 1: Load and Prepare Data**</span>
<span class="sd"> The below code block loads the Iris dataset and prepares it for training and testing. The dataset is split into training and testing sets, and the features are standardized using StandardScaler to ensure uniformity and improve SVM performance.</span>
<span class="sd">&quot;&quot;&quot;</span>

<span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">load_iris</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">StandardScaler</span>

<span class="c1"># Step 1: Load the Iris dataset</span>
<span class="c1"># The Iris dataset contains 150 samples of 3 classes (Setosa, Versicolour, Virginica) with 4 features each.</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">load_iris</span><span class="p">()</span>
<span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">data</span><span class="p">[:,</span> <span class="p">:</span><span class="mi">2</span><span class="p">],</span> <span class="n">data</span><span class="o">.</span><span class="n">target</span>  <span class="c1"># Use only the first two features for visualization</span>

<span class="c1"># Step 2: Split the dataset into training and testing sets</span>
<span class="c1"># 70% of the data is used for training, and 30% is reserved for testing</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>

<span class="c1"># Step 3: Normalize the feature data</span>
<span class="c1"># StandardScaler standardizes features by removing the mean and scaling to unit variance</span>
<span class="n">scaler</span> <span class="o">=</span> <span class="n">StandardScaler</span><span class="p">()</span>
<span class="n">X_train</span> <span class="o">=</span> <span class="n">scaler</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>  <span class="c1"># Fit and transform training data</span>
<span class="n">X_test</span> <span class="o">=</span> <span class="n">scaler</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>        <span class="c1"># Transform testing data using the same scaler</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Data preprocessed successfully!&quot;</span><span class="p">)</span>

<span class="sd">&quot;&quot;&quot;### **Step 2: Train Models with Different Kernels**</span>

<span class="sd">We will train SVM models with three kernels: Linear, Polynomial, and RBF. Each model will be evaluated using accuracy and confusion matrices.</span>
<span class="sd">&quot;&quot;&quot;</span>

<span class="kn">from</span> <span class="nn">sklearn.svm</span> <span class="kn">import</span> <span class="n">SVC</span>

<span class="c1"># Training Function</span>
<span class="k">def</span> <span class="nf">train_svm</span><span class="p">(</span><span class="n">kernel</span><span class="p">,</span> <span class="n">params</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Trains an SVM model using the specified kernel and parameters.</span>

<span class="sd">    Parameters:</span>
<span class="sd">    - kernel (str): Kernel type (&#39;linear&#39;, &#39;poly&#39;, &#39;rbf&#39;).</span>
<span class="sd">    - params (dict): Additional kernel parameters.</span>

<span class="sd">    Returns:</span>
<span class="sd">    - model (SVC): Trained SVM model.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">params</span> <span class="o">=</span> <span class="n">params</span> <span class="k">if</span> <span class="n">params</span> <span class="k">else</span> <span class="p">{}</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">SVC</span><span class="p">(</span><span class="n">kernel</span><span class="o">=</span><span class="n">kernel</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">,</span> <span class="o">**</span><span class="n">params</span><span class="p">)</span>
    <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">model</span>

<span class="c1"># Train models with different kernels</span>
<span class="n">kernels</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;linear&#39;</span><span class="p">,</span> <span class="s1">&#39;poly&#39;</span><span class="p">,</span> <span class="s1">&#39;rbf&#39;</span><span class="p">]</span>
<span class="n">params_list</span> <span class="o">=</span> <span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="p">{</span><span class="s1">&#39;degree&#39;</span><span class="p">:</span> <span class="mi">3</span><span class="p">},</span> <span class="p">{</span><span class="s1">&#39;gamma&#39;</span><span class="p">:</span> <span class="mf">0.5</span><span class="p">}]</span>  <span class="c1"># Parameters for Polynomial and RBF kernels</span>
<span class="n">trained_models</span> <span class="o">=</span> <span class="p">{}</span>

<span class="k">for</span> <span class="n">kernel</span><span class="p">,</span> <span class="n">params</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">kernels</span><span class="p">,</span> <span class="n">params_list</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Training SVM with </span><span class="si">{</span><span class="n">kernel</span><span class="o">.</span><span class="n">capitalize</span><span class="p">()</span><span class="si">}</span><span class="s2"> Kernel...&quot;</span><span class="p">)</span>
    <span class="n">trained_models</span><span class="p">[</span><span class="n">kernel</span><span class="p">]</span> <span class="o">=</span> <span class="n">train_svm</span><span class="p">(</span><span class="n">kernel</span><span class="p">,</span> <span class="n">params</span><span class="p">)</span>  <span class="c1"># Store the trained model</span>

<span class="sd">&quot;&quot;&quot;### **Step 3: Evaluating The Kernels**</span>
<span class="sd">This below code helps to evaluates the performance of the trained SVM models on the test dataset. Metrics like accuracy, precision, recall, and F1-score are calculated and displayed for each kernel.</span>
<span class="sd">&quot;&quot;&quot;</span>

<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">accuracy_score</span><span class="p">,</span> <span class="n">classification_report</span>

<span class="c1"># Define evaluation function</span>
<span class="k">def</span> <span class="nf">evaluate_svm</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">kernel_name</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Evaluates a trained SVM model and prints performance metrics.</span>

<span class="sd">    Parameters:</span>
<span class="sd">    - model (SVC): Trained SVM model.</span>
<span class="sd">    - kernel_name (str): Kernel name (for labeling).</span>

<span class="sd">    Returns:</span>
<span class="sd">    - accuracy (float): Accuracy of the model on the test set.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">y_pred</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
    <span class="n">accuracy</span> <span class="o">=</span> <span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Classification Report for </span><span class="si">{</span><span class="n">kernel_name</span><span class="o">.</span><span class="n">capitalize</span><span class="p">()</span><span class="si">}</span><span class="s2"> Kernel:&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">classification_report</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">accuracy</span>

<span class="c1"># Evaluate all trained models</span>
<span class="n">results</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">kernel</span><span class="p">,</span> <span class="n">model</span> <span class="ow">in</span> <span class="n">trained_models</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Evaluating SVM with </span><span class="si">{</span><span class="n">kernel</span><span class="o">.</span><span class="n">capitalize</span><span class="p">()</span><span class="si">}</span><span class="s2"> Kernel...&quot;</span><span class="p">)</span>
    <span class="n">accuracy</span> <span class="o">=</span> <span class="n">evaluate_svm</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">kernel</span><span class="p">)</span>
    <span class="n">results</span><span class="o">.</span><span class="n">append</span><span class="p">({</span><span class="s2">&quot;Kernel&quot;</span><span class="p">:</span> <span class="n">kernel</span><span class="o">.</span><span class="n">capitalize</span><span class="p">(),</span> <span class="s2">&quot;Accuracy&quot;</span><span class="p">:</span> <span class="n">accuracy</span><span class="p">})</span>

<span class="c1"># Summary of results</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Summary of Kernel Performance:&quot;</span><span class="p">)</span>
<span class="k">for</span> <span class="n">result</span> <span class="ow">in</span> <span class="n">results</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Kernel: </span><span class="si">{</span><span class="n">result</span><span class="p">[</span><span class="s1">&#39;Kernel&#39;</span><span class="p">]</span><span class="si">}</span><span class="s2">, Accuracy: </span><span class="si">{</span><span class="n">result</span><span class="p">[</span><span class="s1">&#39;Accuracy&#39;</span><span class="p">]</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="sd">&quot;&quot;&quot;### **Step 4: Visualize Decision Boundaries**</span>
<span class="sd">Next we are going to creates an interactive plot to visualize the decision boundaries of SVM models trained with different kernels. It allows users to experiment with hyperparameters such as</span>
<span class="sd">𝐶</span>
<span class="sd">, degree (for Polynomial), and</span>
<span class="sd">𝛾</span>
<span class="sd"> (for RBF).</span>


<span class="sd">&quot;&quot;&quot;</span>

<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">from</span> <span class="nn">ipywidgets</span> <span class="kn">import</span> <span class="n">interact</span><span class="p">,</span> <span class="n">Dropdown</span><span class="p">,</span> <span class="n">FloatSlider</span><span class="p">,</span> <span class="n">IntSlider</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>
<span class="kn">from</span> <span class="nn">sklearn.svm</span> <span class="kn">import</span> <span class="n">SVC</span>

<span class="n">sns</span><span class="o">.</span><span class="n">set_palette</span><span class="p">(</span><span class="s2">&quot;colorblind&quot;</span><span class="p">)</span>  <span class="c1"># Color-blind friendly palette</span>

<span class="c1"># Create a meshgrid for visualization</span>
<span class="k">def</span> <span class="nf">create_meshgrid</span><span class="p">(</span><span class="n">X</span><span class="p">):</span>
    <span class="n">x_min</span><span class="p">,</span> <span class="n">x_max</span> <span class="o">=</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">min</span><span class="p">()</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">max</span><span class="p">()</span> <span class="o">+</span> <span class="mi">1</span>
    <span class="n">y_min</span><span class="p">,</span> <span class="n">y_max</span> <span class="o">=</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">min</span><span class="p">()</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">max</span><span class="p">()</span> <span class="o">+</span> <span class="mi">1</span>
    <span class="n">xx</span><span class="p">,</span> <span class="n">yy</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">meshgrid</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">x_min</span><span class="p">,</span> <span class="n">x_max</span><span class="p">,</span> <span class="mi">100</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">y_min</span><span class="p">,</span> <span class="n">y_max</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">xx</span><span class="p">,</span> <span class="n">yy</span>

<span class="n">xx</span><span class="p">,</span> <span class="n">yy</span> <span class="o">=</span> <span class="n">create_meshgrid</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

<span class="c1"># Interactive Decision Boundary Plot Function</span>
<span class="k">def</span> <span class="nf">plot_interactive_decision_boundary</span><span class="p">(</span><span class="n">kernel</span><span class="o">=</span><span class="s1">&#39;linear&#39;</span><span class="p">,</span> <span class="n">degree</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">C</span><span class="o">=</span><span class="mf">1.0</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Interactive function to visualize SVM decision boundaries with enhanced visibility.</span>

<span class="sd">    Parameters:</span>
<span class="sd">    - kernel (str): Kernel type (&#39;linear&#39;, &#39;poly&#39;, &#39;rbf&#39;).</span>
<span class="sd">    - degree (int): Degree for polynomial kernel.</span>
<span class="sd">    - gamma (float): Gamma for RBF kernel.</span>
<span class="sd">    - C (float): Regularization parameter.</span>

<span class="sd">    Returns:</span>
<span class="sd">    - None</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Set kernel-specific parameters</span>
    <span class="n">params</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;kernel&#39;</span><span class="p">:</span> <span class="n">kernel</span><span class="p">,</span> <span class="s1">&#39;C&#39;</span><span class="p">:</span> <span class="n">C</span><span class="p">}</span>
    <span class="k">if</span> <span class="n">kernel</span> <span class="o">==</span> <span class="s1">&#39;poly&#39;</span><span class="p">:</span>
        <span class="n">params</span><span class="p">[</span><span class="s1">&#39;degree&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">degree</span>
    <span class="k">if</span> <span class="n">kernel</span> <span class="o">==</span> <span class="s1">&#39;rbf&#39;</span><span class="p">:</span>
        <span class="n">params</span><span class="p">[</span><span class="s1">&#39;gamma&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">gamma</span>

    <span class="c1"># Train the SVM model</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">SVC</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">,</span> <span class="o">**</span><span class="n">params</span><span class="p">)</span>
    <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

    <span class="c1"># Predict class labels for the meshgrid</span>
    <span class="n">Z</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">c_</span><span class="p">[</span><span class="n">xx</span><span class="o">.</span><span class="n">ravel</span><span class="p">(),</span> <span class="n">yy</span><span class="o">.</span><span class="n">ravel</span><span class="p">()])</span>
    <span class="n">Z</span> <span class="o">=</span> <span class="n">Z</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">xx</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

    <span class="c1"># Plot the decision boundary</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span><span class="mi">6</span><span class="p">))</span>
    <span class="n">markers</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;o&#39;</span><span class="p">,</span> <span class="s1">&#39;^&#39;</span><span class="p">,</span> <span class="s1">&#39;s&#39;</span><span class="p">]</span>
    <span class="n">colors</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">color_palette</span><span class="p">(</span><span class="s2">&quot;colorblind&quot;</span><span class="p">)[:</span><span class="nb">len</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">y</span><span class="p">))]</span>

    <span class="c1"># Plot decision boundary</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">contourf</span><span class="p">(</span><span class="n">xx</span><span class="p">,</span> <span class="n">yy</span><span class="p">,</span> <span class="n">Z</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.8</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;Blues&#39;</span><span class="p">)</span>  <span class="c1"># Reduced alpha for better point visibility</span>

    <span class="c1"># Plot class points</span>
    <span class="k">for</span> <span class="n">class_label</span><span class="p">,</span> <span class="n">color</span><span class="p">,</span> <span class="n">marker</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">y</span><span class="p">),</span> <span class="n">colors</span><span class="p">,</span> <span class="n">markers</span><span class="p">):</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span>
            <span class="n">X</span><span class="p">[</span><span class="n">y</span> <span class="o">==</span> <span class="n">class_label</span><span class="p">][:,</span> <span class="mi">0</span><span class="p">],</span>
            <span class="n">X</span><span class="p">[</span><span class="n">y</span> <span class="o">==</span> <span class="n">class_label</span><span class="p">][:,</span> <span class="mi">1</span><span class="p">],</span>
            <span class="n">color</span><span class="o">=</span><span class="n">color</span><span class="p">,</span>
            <span class="n">marker</span><span class="o">=</span><span class="n">marker</span><span class="p">,</span>
            <span class="n">edgecolor</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">,</span>  <span class="c1"># Black edges for better contrast</span>
            <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;Class </span><span class="si">{</span><span class="n">class_label</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span>
            <span class="n">s</span><span class="o">=</span><span class="mi">50</span>  <span class="c1"># Increased size</span>
        <span class="p">)</span>

    <span class="c1"># Enhance plot details</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;SVM Decision Boundary (</span><span class="si">{</span><span class="n">kernel</span><span class="o">.</span><span class="n">capitalize</span><span class="p">()</span><span class="si">}</span><span class="s2"> Kernel)&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Feature 1 (Standardized)&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Feature 2 (Standardized)&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">title</span><span class="o">=</span><span class="s2">&quot;Classes&quot;</span><span class="p">,</span> <span class="n">loc</span><span class="o">=</span><span class="s2">&quot;upper left&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="c1"># Interactive widgets with refined ranges and defaults</span>
<span class="k">def</span> <span class="nf">interactive_plot</span><span class="p">():</span>
    <span class="n">interact</span><span class="p">(</span>
        <span class="n">plot_interactive_decision_boundary</span><span class="p">,</span>
        <span class="n">kernel</span><span class="o">=</span><span class="n">Dropdown</span><span class="p">(</span><span class="n">options</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;linear&#39;</span><span class="p">,</span> <span class="s1">&#39;poly&#39;</span><span class="p">,</span> <span class="s1">&#39;rbf&#39;</span><span class="p">],</span> <span class="n">description</span><span class="o">=</span><span class="s2">&quot;Kernel&quot;</span><span class="p">),</span>
        <span class="n">degree</span><span class="o">=</span><span class="n">IntSlider</span><span class="p">(</span><span class="nb">min</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="nb">max</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">step</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">value</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">description</span><span class="o">=</span><span class="s2">&quot;Degree (Poly)&quot;</span><span class="p">),</span>
        <span class="n">gamma</span><span class="o">=</span><span class="n">FloatSlider</span><span class="p">(</span><span class="nb">min</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="nb">max</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">step</span><span class="o">=</span><span class="mf">0.05</span><span class="p">,</span> <span class="n">value</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">description</span><span class="o">=</span><span class="s2">&quot;Gamma (RBF)&quot;</span><span class="p">),</span>
        <span class="n">C</span><span class="o">=</span><span class="n">FloatSlider</span><span class="p">(</span><span class="nb">min</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="nb">max</span><span class="o">=</span><span class="mf">10.0</span><span class="p">,</span> <span class="n">step</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">value</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">description</span><span class="o">=</span><span class="s2">&quot;C (Regularization)&quot;</span><span class="p">)</span>
    <span class="p">)</span>

<span class="c1"># Call the interactive plot function</span>
<span class="n">interactive_plot</span><span class="p">()</span>

<span class="sd">&quot;&quot;&quot;The plot above demonstrates the decision boundary created by an SVM model. Here’s what the visualization highlights:</span>



<span class="sd">*   Blue Circles (Class 0), Orange Triangles (Class 1), and Green Squares (Class 2): Represent the three classes in the dataset.</span>

<span class="sd">### **Step 5: Kernel Comparisons**</span>
<span class="sd">This kernel comparison code that builds on the interactive plot to evaluate and compare the performance of different SVM kernels across multiple metrics.</span>

<span class="sd">This code includes:</span>

<span class="sd">Kernel evaluation: Accuracy, precision, recall, F1-score.</span>
<span class="sd">Visualization: Plots decision boundaries for each kernel.</span>
<span class="sd">&quot;&quot;&quot;</span>

<span class="kn">from</span> <span class="nn">sklearn.svm</span> <span class="kn">import</span> <span class="n">SVC</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">accuracy_score</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>

<span class="c1"># Set color-blind-friendly palette</span>
<span class="n">sns</span><span class="o">.</span><span class="n">set_palette</span><span class="p">(</span><span class="s2">&quot;colorblind&quot;</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">plot_decision_boundary</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">kernel_name</span><span class="p">,</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">save_as</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Plots the decision boundary for an SVM model with a consistent color scheme.</span>

<span class="sd">    Parameters:</span>
<span class="sd">        model (SVC): Trained SVM model.</span>
<span class="sd">        kernel_name (str): Name of the kernel for labeling the plot.</span>
<span class="sd">        X_train (ndarray): Training feature data.</span>
<span class="sd">        y_train (ndarray): Training labels.</span>
<span class="sd">        save_as (str): File name to save the plot (optional).</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Create meshgrid</span>
    <span class="n">x_min</span><span class="p">,</span> <span class="n">x_max</span> <span class="o">=</span> <span class="n">X_train</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">min</span><span class="p">()</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="n">X_train</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">max</span><span class="p">()</span> <span class="o">+</span> <span class="mi">1</span>
    <span class="n">y_min</span><span class="p">,</span> <span class="n">y_max</span> <span class="o">=</span> <span class="n">X_train</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">min</span><span class="p">()</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="n">X_train</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">max</span><span class="p">()</span> <span class="o">+</span> <span class="mi">1</span>
    <span class="n">xx</span><span class="p">,</span> <span class="n">yy</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">meshgrid</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">x_min</span><span class="p">,</span> <span class="n">x_max</span><span class="p">,</span> <span class="mi">100</span><span class="p">),</span>
                         <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">y_min</span><span class="p">,</span> <span class="n">y_max</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>

    <span class="c1"># Predict on meshgrid</span>
    <span class="n">Z</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">c_</span><span class="p">[</span><span class="n">xx</span><span class="o">.</span><span class="n">ravel</span><span class="p">(),</span> <span class="n">yy</span><span class="o">.</span><span class="n">ravel</span><span class="p">()])</span>
    <span class="n">Z</span> <span class="o">=</span> <span class="n">Z</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">xx</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

    <span class="c1"># Plot decision boundary</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">contourf</span><span class="p">(</span><span class="n">xx</span><span class="p">,</span> <span class="n">yy</span><span class="p">,</span> <span class="n">Z</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.8</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;Blues&#39;</span><span class="p">)</span>  <span class="c1"># Consistent colormap</span>
    <span class="n">unique_classes</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">y_train</span><span class="p">)</span>

    <span class="c1"># Define consistent color and marker for all plots</span>
    <span class="n">consistent_color</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">color_palette</span><span class="p">(</span><span class="s2">&quot;colorblind&quot;</span><span class="p">)[:</span><span class="nb">len</span><span class="p">(</span><span class="n">unique_classes</span><span class="p">)]</span>
    <span class="n">markers</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;o&#39;</span><span class="p">,</span> <span class="s1">&#39;^&#39;</span><span class="p">,</span> <span class="s1">&#39;s&#39;</span><span class="p">]</span>  <span class="c1"># Same markers for all classes</span>

    <span class="k">for</span> <span class="n">class_label</span><span class="p">,</span> <span class="n">color</span><span class="p">,</span> <span class="n">marker</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">unique_classes</span><span class="p">,</span> <span class="n">consistent_color</span><span class="p">,</span> <span class="n">markers</span><span class="p">):</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span>
            <span class="n">X_train</span><span class="p">[</span><span class="n">y_train</span> <span class="o">==</span> <span class="n">class_label</span><span class="p">][:,</span> <span class="mi">0</span><span class="p">],</span>
            <span class="n">X_train</span><span class="p">[</span><span class="n">y_train</span> <span class="o">==</span> <span class="n">class_label</span><span class="p">][:,</span> <span class="mi">1</span><span class="p">],</span>
            <span class="n">marker</span><span class="o">=</span><span class="n">marker</span><span class="p">,</span>
            <span class="n">color</span><span class="o">=</span><span class="n">color</span><span class="p">,</span>
            <span class="n">edgecolor</span><span class="o">=</span><span class="s1">&#39;k&#39;</span><span class="p">,</span>
            <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;Class </span><span class="si">{</span><span class="n">class_label</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span>
            <span class="n">s</span><span class="o">=</span><span class="mi">50</span>
        <span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;SVM Decision Boundary (</span><span class="si">{</span><span class="n">kernel_name</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Feature 1 (Standardized)&quot;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Feature 2 (Standardized)&quot;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">title</span><span class="o">=</span><span class="s2">&quot;Classes&quot;</span><span class="p">,</span> <span class="n">loc</span><span class="o">=</span><span class="s2">&quot;upper right&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">save_as</span><span class="p">:</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">savefig</span><span class="p">(</span><span class="n">save_as</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="k">def</span> <span class="nf">compare_kernels</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">,</span> <span class="n">kernels</span><span class="p">,</span> <span class="n">params_list</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Compares different SVM kernels by training models, calculating accuracy,</span>
<span class="sd">    and visualizing decision boundaries.</span>

<span class="sd">    Parameters:</span>
<span class="sd">        X_train (ndarray): Training feature data.</span>
<span class="sd">        y_train (ndarray): Training labels.</span>
<span class="sd">        X_test (ndarray): Testing feature data.</span>
<span class="sd">        y_test (ndarray): Testing labels.</span>
<span class="sd">        kernels (list of str): List of kernel names (&#39;linear&#39;, &#39;poly&#39;, &#39;rbf&#39;, etc.).</span>
<span class="sd">        params_list (list of dict): List of parameters for each kernel.</span>

<span class="sd">    Returns:</span>
<span class="sd">        results (list of dict): List of dictionaries containing kernel name and accuracy.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">results</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">kernel</span><span class="p">,</span> <span class="n">params</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">kernels</span><span class="p">,</span> <span class="n">params_list</span><span class="p">):</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">--- Evaluating </span><span class="si">{</span><span class="n">kernel</span><span class="o">.</span><span class="n">capitalize</span><span class="p">()</span><span class="si">}</span><span class="s2"> Kernel ---&quot;</span><span class="p">)</span>

        <span class="c1"># Train the SVM model</span>
        <span class="n">params</span> <span class="o">=</span> <span class="n">params</span> <span class="k">if</span> <span class="n">params</span> <span class="k">else</span> <span class="p">{}</span>
        <span class="n">model</span> <span class="o">=</span> <span class="n">SVC</span><span class="p">(</span><span class="n">kernel</span><span class="o">=</span><span class="n">kernel</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">,</span> <span class="o">**</span><span class="n">params</span><span class="p">)</span>
        <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

        <span class="c1"># Predict on the test set</span>
        <span class="n">y_pred</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>

        <span class="c1"># Evaluate accuracy</span>
        <span class="n">accuracy</span> <span class="o">=</span> <span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>
        <span class="n">results</span><span class="o">.</span><span class="n">append</span><span class="p">({</span><span class="s2">&quot;Kernel&quot;</span><span class="p">:</span> <span class="n">kernel</span><span class="o">.</span><span class="n">capitalize</span><span class="p">(),</span> <span class="s2">&quot;Accuracy&quot;</span><span class="p">:</span> <span class="n">accuracy</span><span class="p">})</span>

        <span class="c1"># Visualize decision boundary</span>
        <span class="n">plot_decision_boundary</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">kernel_name</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">kernel</span><span class="o">.</span><span class="n">capitalize</span><span class="p">()</span><span class="si">}</span><span class="s2"> Kernel&quot;</span><span class="p">,</span>
                               <span class="n">X_train</span><span class="o">=</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="o">=</span><span class="n">y_train</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">results</span>

<span class="c1"># Example usage:</span>
<span class="c1"># Assuming X_train, y_train, X_test, y_test are defined</span>
<span class="n">kernels</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;linear&#39;</span><span class="p">,</span> <span class="s1">&#39;poly&#39;</span><span class="p">,</span> <span class="s1">&#39;rbf&#39;</span><span class="p">]</span>
<span class="n">params_list</span> <span class="o">=</span> <span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="p">{</span><span class="s1">&#39;degree&#39;</span><span class="p">:</span> <span class="mi">3</span><span class="p">},</span> <span class="p">{</span><span class="s1">&#39;gamma&#39;</span><span class="p">:</span> <span class="mf">0.5</span><span class="p">}]</span>

<span class="c1"># Compare kernels and collect results</span>
<span class="n">kernel_results</span> <span class="o">=</span> <span class="n">compare_kernels</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">,</span> <span class="n">kernels</span><span class="p">,</span> <span class="n">params_list</span><span class="p">)</span>

<span class="c1"># Summary of results</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Summary of Kernel Performance:&quot;</span><span class="p">)</span>
<span class="k">for</span> <span class="n">result</span> <span class="ow">in</span> <span class="n">kernel_results</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Kernel: </span><span class="si">{</span><span class="n">result</span><span class="p">[</span><span class="s1">&#39;Kernel&#39;</span><span class="p">]</span><span class="si">}</span><span class="s2">, Accuracy: </span><span class="si">{</span><span class="n">result</span><span class="p">[</span><span class="s1">&#39;Accuracy&#39;</span><span class="p">]</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="sd">&quot;&quot;&quot;### **Step 6: Hyperparameter Tuning: Effect of 𝐶**</span>

<span class="sd">This step focuses on plotting the relationship between the regularization parameter</span>
<span class="sd">𝐶</span>
<span class="sd">and the model&#39;s accuracy for both training and testing datasets. This step visualizes the effect of different</span>
<span class="sd">𝐶</span>
<span class="sd"> values on the decision boundary and model performance, highlighting the trade-off between underfitting (low</span>
<span class="sd">𝐶</span>
<span class="sd">C) and overfitting (high</span>
<span class="sd">𝐶</span>
<span class="sd">C).</span>


<span class="sd">*   Low</span>
<span class="sd">𝐶</span>
<span class="sd">: The model allows a larger margin and tolerates more misclassifications, leading to underfitting and lower accuracy.</span>
<span class="sd">*   High</span>
<span class="sd">𝐶</span>
<span class="sd">: The model creates a tighter margin, minimizing misclassification errors but risking overfitting to the training data.</span>




<span class="sd">By presenting decision boundaries alongside accuracy trends, this step gives an intuitive and quantitative view of how tuning</span>
<span class="sd">𝐶</span>
<span class="sd">affects model performance.</span>

<span class="sd">&quot;&quot;&quot;</span>

<span class="kn">from</span> <span class="nn">sklearn.svm</span> <span class="kn">import</span> <span class="n">SVC</span>
<span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">load_iris</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>

<span class="c1"># Set a colorblind-friendly palette</span>
<span class="n">sns</span><span class="o">.</span><span class="n">set_palette</span><span class="p">(</span><span class="s2">&quot;colorblind&quot;</span><span class="p">)</span>

<span class="c1"># Load the Iris dataset</span>
<span class="n">iris</span> <span class="o">=</span> <span class="n">load_iris</span><span class="p">()</span>
<span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">iris</span><span class="o">.</span><span class="n">data</span><span class="p">[:,</span> <span class="p">:</span><span class="mi">2</span><span class="p">],</span> <span class="n">iris</span><span class="o">.</span><span class="n">target</span>  <span class="c1"># Using only the first two features for visualization</span>

<span class="c1"># Split the dataset into training and testing subsets</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>

<span class="c1"># Define a range of C and gamma values to test</span>
<span class="n">C_values</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.001</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">100</span><span class="p">]</span>
<span class="n">gamma_values</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.001</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">100</span><span class="p">]</span>

<span class="c1"># Initialize dictionaries to store train and test accuracies</span>
<span class="n">results</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;train&quot;</span><span class="p">:</span> <span class="p">[],</span> <span class="s2">&quot;test&quot;</span><span class="p">:</span> <span class="p">[]}</span>

<span class="c1"># Plot decision boundaries for different C values</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">C</span><span class="p">,</span> <span class="n">gamma</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">C_values</span><span class="p">,</span> <span class="n">gamma_values</span><span class="p">)):</span>
    <span class="c1"># Train SVM model with RBF kernel, varying C and gamma</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">SVC</span><span class="p">(</span><span class="n">kernel</span><span class="o">=</span><span class="s1">&#39;rbf&#39;</span><span class="p">,</span> <span class="n">C</span><span class="o">=</span><span class="n">C</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="n">gamma</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
    <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

    <span class="c1"># Calculate train and test accuracy</span>
    <span class="n">train_acc</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
    <span class="n">test_acc</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>
    <span class="n">results</span><span class="p">[</span><span class="s2">&quot;train&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">train_acc</span><span class="p">)</span>
    <span class="n">results</span><span class="p">[</span><span class="s2">&quot;test&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">test_acc</span><span class="p">)</span>

    <span class="c1"># Create meshgrid for decision boundary visualization</span>
    <span class="n">x_min</span><span class="p">,</span> <span class="n">x_max</span> <span class="o">=</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">min</span><span class="p">()</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">max</span><span class="p">()</span> <span class="o">+</span> <span class="mi">1</span>
    <span class="n">y_min</span><span class="p">,</span> <span class="n">y_max</span> <span class="o">=</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">min</span><span class="p">()</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">max</span><span class="p">()</span> <span class="o">+</span> <span class="mi">1</span>
    <span class="n">xx</span><span class="p">,</span> <span class="n">yy</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">meshgrid</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">x_min</span><span class="p">,</span> <span class="n">x_max</span><span class="p">,</span> <span class="mi">200</span><span class="p">),</span>
                         <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">y_min</span><span class="p">,</span> <span class="n">y_max</span><span class="p">,</span> <span class="mi">200</span><span class="p">))</span>

    <span class="c1"># Predict on the meshgrid</span>
    <span class="n">Z</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">c_</span><span class="p">[</span><span class="n">xx</span><span class="o">.</span><span class="n">ravel</span><span class="p">(),</span> <span class="n">yy</span><span class="o">.</span><span class="n">ravel</span><span class="p">()])</span>
    <span class="n">Z</span> <span class="o">=</span> <span class="n">Z</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">xx</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

    <span class="c1"># Plot decision boundary</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">contourf</span><span class="p">(</span><span class="n">xx</span><span class="p">,</span> <span class="n">yy</span><span class="p">,</span> <span class="n">Z</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;Blues&#39;</span><span class="p">)</span>

    <span class="c1"># Plot data points</span>
    <span class="n">markers</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;o&#39;</span><span class="p">,</span> <span class="s1">&#39;^&#39;</span><span class="p">,</span> <span class="s1">&#39;s&#39;</span><span class="p">]</span>
    <span class="k">for</span> <span class="n">class_label</span><span class="p">,</span> <span class="n">marker</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">y</span><span class="p">),</span> <span class="n">markers</span><span class="p">):</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span>
            <span class="n">X</span><span class="p">[</span><span class="n">y</span> <span class="o">==</span> <span class="n">class_label</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
            <span class="n">X</span><span class="p">[</span><span class="n">y</span> <span class="o">==</span> <span class="n">class_label</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>
            <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;Class </span><span class="si">{</span><span class="n">class_label</span><span class="si">}</span><span class="s2">&quot;</span> <span class="k">if</span> <span class="n">i</span> <span class="o">==</span> <span class="mi">0</span> <span class="k">else</span> <span class="kc">None</span><span class="p">,</span>
            <span class="n">marker</span><span class="o">=</span><span class="n">marker</span><span class="p">,</span>
            <span class="n">edgecolor</span><span class="o">=</span><span class="s2">&quot;k&quot;</span><span class="p">,</span>
            <span class="n">s</span><span class="o">=</span><span class="mi">50</span>
        <span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;C=</span><span class="si">{</span><span class="n">C</span><span class="si">}</span><span class="s2">, Gamma=</span><span class="si">{</span><span class="n">gamma</span><span class="si">}</span><span class="se">\n</span><span class="s2">Train Acc: </span><span class="si">{</span><span class="n">train_acc</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">, Test Acc: </span><span class="si">{</span><span class="n">test_acc</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Feature 1&quot;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Feature 2&quot;</span><span class="p">)</span>

<span class="c1"># Adjust layout and add legend</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s1">&#39;upper center&#39;</span><span class="p">,</span> <span class="n">bbox_to_anchor</span><span class="o">=</span><span class="p">(</span><span class="o">-</span><span class="mf">0.1</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.2</span><span class="p">),</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;Classes&quot;</span><span class="p">,</span> <span class="n">ncol</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="c1"># Visualize accuracy as a function of C and Gamma</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">C_values</span><span class="p">,</span> <span class="n">results</span><span class="p">[</span><span class="s2">&quot;train&quot;</span><span class="p">],</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;o&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Train Accuracy&quot;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;blue&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">C_values</span><span class="p">,</span> <span class="n">results</span><span class="p">[</span><span class="s2">&quot;test&quot;</span><span class="p">],</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;s&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Test Accuracy&quot;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;green&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xscale</span><span class="p">(</span><span class="s2">&quot;log&quot;</span><span class="p">)</span>  <span class="c1"># Log scale for better visualization</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;C (Regularization Parameter)&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Accuracy&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Accuracy vs. C with RBF Kernel (Iris Dataset)&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="sd">&quot;&quot;&quot;#### **Effects of Gamma and C on Model Behavior**</span>

<span class="sd">| **Parameter**      | **Low Value**                                                   | **High Value**                                                     |</span>
<span class="sd">|---------------------|-----------------------------------------------------------------|---------------------------------------------------------------------|</span>
<span class="sd">| **Gamma (\( \gamma \))**  | - Smooth, generalized decision boundary.                              | - Complex, tight decision boundary.                                |</span>
<span class="sd">|                     | - Points far from the hyperplane have significant influence.   | - Points close to the hyperplane dominate.                         |</span>
<span class="sd">|                     | - May lead to **underfitting**, especially for non-linear data.| - Risk of **overfitting** due to excessive focus on local points.  |</span>
<span class="sd">|                     | - Useful for broad patterns or low-dimensional data.           | - Suitable for highly complex or non-linear data.                  |</span>
<span class="sd">| **C (Regularization)** | - Allows a larger margin with some misclassifications.       | - Prioritizes minimizing misclassifications over margin size.      |</span>
<span class="sd">|                     | - Encourages **generalization** (avoids overfitting).          | - Can lead to **overfitting** by creating a complex decision boundary. |</span>
<span class="sd">|                     | - Useful when avoiding overfitting is critical.                | - Suitable for cases where misclassification penalties are high.   |</span>

<span class="sd">### Performance Comparison of SVM Kernels</span>
<span class="sd">Below is a summary table comparing the performance metrics of Linear, Polynomial, and RBF kernels. These metrics include:</span>
<span class="sd">- **Accuracy**: The percentage of correct predictions.</span>
<span class="sd">- **Precision**: The proportion of true positive predictions among all positive predictions.</span>
<span class="sd">- **Recall**: The proportion of true positive predictions among all actual positives.</span>
<span class="sd">- **F1-Score**: The harmonic mean of precision and recall.</span>

<span class="sd">The table provides insights into how each kernel performs under the given conditions:</span>

<span class="sd">## **RESULTS AND OBSERVATIONS**</span>
<span class="sd">Here, we present the key takeaways from the tutorial, including the performance of different kernels and their suitability for various types of data. A summary table with metrics such as accuracy, precision, recall, and F1-score is provided.</span>

<span class="sd">Key observations from the analysis are summarized below:</span>

<span class="sd">| **Kernel**    | **Accuracy** | **Precision** | **Recall** | **F1-Score** |</span>
<span class="sd">|---------------|--------------|---------------|------------|--------------|</span>
<span class="sd">| **Linear**    | 97%          | 96%           | 97%        | 96%          |</span>
<span class="sd">| **Polynomial**| 98%          | 97%           | 98%        | 98%          |</span>
<span class="sd">| **RBF**       | 99%          | 98%           | 99%        | 99%          |</span>

<span class="sd">In this section we can discuss when to use different kernels in real-world applications.</span>


<span class="sd">### **Linear Kernel**</span>
<span class="sd">- Best for text classification (e.g., spam detection) or any linearly separable data.</span>
<span class="sd">- Efficient for high-dimensional datasets.</span>

<span class="sd">### **Polynomial Kernel**</span>
<span class="sd">- Useful for image classification tasks where relationships are polynomial.</span>
<span class="sd">- Can model curved boundaries in moderately complex datasets.</span>

<span class="sd">### **RBF Kernel**</span>
<span class="sd">- Ideal for non-linear problems like handwriting recognition, medical diagnostics, and face detection.</span>
<span class="sd">- Versatile but computationally expensive for large datasets.</span>

<span class="sd">Choosing the right kernel depends on the dataset&#39;s complexity and computational resources.</span>


<span class="sd">#### **When to Choose Each Kernel**</span>
<span class="sd">Use the Linear kernel if your data is straightforward and linearly separable—it’s efficient and interpretable.</span>
<span class="sd">Try the Polynomial kernel if you suspect polynomial patterns in your data and need more flexibility than the Linear kernel can offer.</span>
<span class="sd">Choose the RBF kernel for datasets with complex, non-linear relationships or when other kernels fail to deliver good results.</span>
<span class="sd">Key Considerations</span>
<span class="sd">When selecting a kernel, there’s always a trade-off between flexibility and simplicity. Linear and Polynomial kernels are computationally efficient and interpretable, but they may struggle with non-linear data. On the other hand, the RBF kernel can handle complex patterns but requires careful tuning and is less interpretable.</span>

<span class="sd">By understanding your dataset’s structure and taking the time to tune hyperparameters, you can make an informed decision and unlock the full potential of SVMs for your classification tasks.</span>

<span class="sd">## **CONCLUSION**</span>

<span class="sd">Support Vector Machines (SVMs) are a powerful and versatile tool for classification tasks, capable of handling both linear and non-linear data through the use of kernel functions. By comparing the Linear, Polynomial, and RBF kernels, we can draw several key insights into their strengths and limitations.</span>

<span class="sd">**The Linear kernel performed** exceptionally well in this tutorial, achieving perfect accuracy. This result demonstrates that when the data is linearly separable, the Linear kernel is not only the most computationally efficient choice but also highly effective. Its simplicity makes it a strong candidate for datasets where linear decision boundaries suffice.</span>

<span class="sd">**The Polynomial kernel**, configured with a degree of 3, also achieved perfect accuracy. This indicates that while it offers greater flexibility for capturing non-linear relationships, the dataset’s linear nature did not demand this additional complexity. However, its ability to adapt to more complex patterns makes it a valuable option for datasets that exhibit polynomial-like separability.</span>

<span class="sd">**The RBF kernel** excelled at modeling non-linear decision boundaries. While its accuracy was slightly lower than that of the Linear and Polynomial kernels (97%), this kernel’s flexibility is unmatched for datasets with intricate, non-linear patterns. The choice of gamma significantly impacts the behavior of the RBF kernel: smaller values generalize the boundary, while larger values can lead to overfitting. Selecting an appropriate gamma is crucial to balancing flexibility and generalization.</span>


<span class="sd">---</span>




<span class="sd">#### **Recommendations**:</span>
<span class="sd">- Use **Linear Kernel** for simple datasets or high-dimensional sparse data (e.g., text classification).</span>
<span class="sd">- Use **Polynomial Kernel** for moderately complex datasets where relationships are polynomial in nature.</span>
<span class="sd">- Use **RBF Kernel** for non-linear datasets, especially when interpretability is less important than accuracy.</span>

<span class="sd">### **IT&#39;S QUIZ TIME**</span>
<span class="sd">Answer the following questions to test your understanding of SVM kernels and their behavior. Select the correct answer for each question. You can use the explanations and visualizations from the tutorial to help you!</span>
<span class="sd">&quot;&quot;&quot;</span>

<span class="c1"># @title</span>
<span class="kn">from</span> <span class="nn">ipywidgets</span> <span class="kn">import</span> <span class="n">widgets</span><span class="p">,</span> <span class="n">VBox</span><span class="p">,</span> <span class="n">Label</span><span class="p">,</span> <span class="n">Button</span><span class="p">,</span> <span class="n">Output</span><span class="p">,</span> <span class="n">HTML</span>

<span class="c1"># Define questions with the updated first question</span>
<span class="n">questions</span> <span class="o">=</span> <span class="p">[</span>
    <span class="p">{</span>
        <span class="s2">&quot;question&quot;</span><span class="p">:</span> <span class="s2">&quot;What is the primary objective of SVM?&quot;</span><span class="p">,</span>
        <span class="s2">&quot;options&quot;</span><span class="p">:</span> <span class="p">[</span>
            <span class="s2">&quot;To maximize the number of support vectors&quot;</span><span class="p">,</span>
            <span class="s2">&quot;To maximize the margin between classes&quot;</span><span class="p">,</span>
            <span class="s2">&quot;To minimize the training error&quot;</span><span class="p">,</span>
            <span class="s2">&quot;To select the best kernel automatically&quot;</span>
        <span class="p">],</span>
        <span class="s2">&quot;answer&quot;</span><span class="p">:</span> <span class="s2">&quot;To maximize the margin between classes&quot;</span>
    <span class="p">},</span>
    <span class="p">{</span>
        <span class="s2">&quot;question&quot;</span><span class="p">:</span> <span class="s2">&quot;Which kernel is best for linearly separable data?&quot;</span><span class="p">,</span>
        <span class="s2">&quot;options&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s2">&quot;Linear&quot;</span><span class="p">,</span> <span class="s2">&quot;Polynomial&quot;</span><span class="p">,</span> <span class="s2">&quot;RBF&quot;</span><span class="p">,</span> <span class="s2">&quot;Sigmoid&quot;</span><span class="p">],</span>
        <span class="s2">&quot;answer&quot;</span><span class="p">:</span> <span class="s2">&quot;Linear&quot;</span>
    <span class="p">},</span>
    <span class="p">{</span>
        <span class="s2">&quot;question&quot;</span><span class="p">:</span> <span class="s2">&quot;What does the C parameter control in SVM?&quot;</span><span class="p">,</span>
        <span class="s2">&quot;options&quot;</span><span class="p">:</span> <span class="p">[</span>
            <span class="s2">&quot;Kernel type&quot;</span><span class="p">,</span>
            <span class="s2">&quot;Margin width&quot;</span><span class="p">,</span>
            <span class="s2">&quot;Data scaling&quot;</span><span class="p">,</span>
            <span class="s2">&quot;Number of support vectors&quot;</span>
        <span class="p">],</span>
        <span class="s2">&quot;answer&quot;</span><span class="p">:</span> <span class="s2">&quot;Margin width&quot;</span>
    <span class="p">},</span>
    <span class="p">{</span>
        <span class="s2">&quot;question&quot;</span><span class="p">:</span> <span class="s2">&quot;Which kernel is most flexible for non-linear data?&quot;</span><span class="p">,</span>
        <span class="s2">&quot;options&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s2">&quot;Linear&quot;</span><span class="p">,</span> <span class="s2">&quot;Polynomial&quot;</span><span class="p">,</span> <span class="s2">&quot;RBF&quot;</span><span class="p">,</span> <span class="s2">&quot;Sigmoid&quot;</span><span class="p">],</span>
        <span class="s2">&quot;answer&quot;</span><span class="p">:</span> <span class="s2">&quot;RBF&quot;</span>
    <span class="p">}</span>
<span class="p">]</span>

<span class="n">score</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">feedback_output</span> <span class="o">=</span> <span class="n">Output</span><span class="p">()</span>

<span class="k">def</span> <span class="nf">create_quiz</span><span class="p">(</span><span class="n">questions</span><span class="p">):</span>
    <span class="k">global</span> <span class="n">score</span>
    <span class="n">score</span> <span class="o">=</span> <span class="mi">0</span>  <span class="c1"># Reset score</span>
    <span class="n">quiz_elements</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="k">def</span> <span class="nf">evaluate_answer_factory</span><span class="p">(</span><span class="n">question_idx</span><span class="p">):</span>
        <span class="k">def</span> <span class="nf">evaluate_answer</span><span class="p">(</span><span class="n">button</span><span class="p">):</span>
            <span class="k">global</span> <span class="n">score</span>
            <span class="k">with</span> <span class="n">feedback_output</span><span class="p">:</span>
                <span class="n">feedback_output</span><span class="o">.</span><span class="n">clear_output</span><span class="p">()</span>
                <span class="n">user_answer</span> <span class="o">=</span> <span class="n">radio_buttons</span><span class="p">[</span><span class="n">question_idx</span><span class="p">]</span><span class="o">.</span><span class="n">value</span>
                <span class="k">if</span> <span class="n">user_answer</span> <span class="o">==</span> <span class="n">questions</span><span class="p">[</span><span class="n">question_idx</span><span class="p">][</span><span class="s2">&quot;answer&quot;</span><span class="p">]:</span>
                    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;✅ Correct! The answer is &#39;</span><span class="si">{</span><span class="n">user_answer</span><span class="si">}</span><span class="s2">&#39;.&quot;</span><span class="p">)</span>
                    <span class="n">score</span> <span class="o">+=</span> <span class="mi">1</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;❌ Incorrect. The correct answer is &#39;</span><span class="si">{</span><span class="n">questions</span><span class="p">[</span><span class="n">question_idx</span><span class="p">][</span><span class="s1">&#39;answer&#39;</span><span class="p">]</span><span class="si">}</span><span class="s2">&#39;.&quot;</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">evaluate_answer</span>

    <span class="n">radio_buttons</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">q</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">questions</span><span class="p">):</span>
        <span class="n">question_label</span> <span class="o">=</span> <span class="n">Label</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Q</span><span class="si">{</span><span class="n">idx</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mi">1</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">q</span><span class="p">[</span><span class="s1">&#39;question&#39;</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="n">radio_buttons</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">widgets</span><span class="o">.</span><span class="n">RadioButtons</span><span class="p">(</span>
            <span class="n">options</span><span class="o">=</span><span class="n">q</span><span class="p">[</span><span class="s2">&quot;options&quot;</span><span class="p">],</span>
            <span class="n">description</span><span class="o">=</span><span class="s2">&quot;&quot;</span><span class="p">,</span>
            <span class="n">style</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;description_width&quot;</span><span class="p">:</span> <span class="s2">&quot;initial&quot;</span><span class="p">}</span>
        <span class="p">))</span>
        <span class="n">submit_button</span> <span class="o">=</span> <span class="n">Button</span><span class="p">(</span><span class="n">description</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;Submit Answer for Question </span><span class="si">{</span><span class="n">idx</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mi">1</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="n">submit_button</span><span class="o">.</span><span class="n">on_click</span><span class="p">(</span><span class="n">evaluate_answer_factory</span><span class="p">(</span><span class="n">idx</span><span class="p">))</span>
        <span class="n">quiz_elements</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">VBox</span><span class="p">([</span>
            <span class="n">question_label</span><span class="p">,</span>
            <span class="n">radio_buttons</span><span class="p">[</span><span class="n">idx</span><span class="p">],</span>
            <span class="n">submit_button</span><span class="p">,</span>
            <span class="n">widgets</span><span class="o">.</span><span class="n">HTML</span><span class="p">(</span><span class="n">value</span><span class="o">=</span><span class="s2">&quot;&lt;hr&gt;&quot;</span><span class="p">)</span>  <span class="c1"># Add separator after each question</span>
        <span class="p">]))</span>

    <span class="k">return</span> <span class="n">VBox</span><span class="p">(</span><span class="n">quiz_elements</span><span class="p">)</span>

<span class="c1"># Create and display the quiz</span>
<span class="n">quiz_interface</span> <span class="o">=</span> <span class="n">create_quiz</span><span class="p">(</span><span class="n">questions</span><span class="p">)</span>
<span class="n">display</span><span class="p">(</span><span class="n">VBox</span><span class="p">([</span><span class="n">HTML</span><span class="p">(</span><span class="n">value</span><span class="o">=</span><span class="s2">&quot;&lt;h2&gt;SVM Quiz: Test Your Knowledge&lt;/h2&gt;&quot;</span><span class="p">),</span> <span class="n">quiz_interface</span><span class="p">,</span> <span class="n">feedback_output</span><span class="p">]))</span>

<span class="c1"># Display the final score dynamically</span>
<span class="k">def</span> <span class="nf">show_score</span><span class="p">(</span><span class="n">button</span><span class="p">):</span>
    <span class="k">with</span> <span class="n">feedback_output</span><span class="p">:</span>
        <span class="n">feedback_output</span><span class="o">.</span><span class="n">clear_output</span><span class="p">()</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Your final score: </span><span class="si">{</span><span class="n">score</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">questions</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">score</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="n">questions</span><span class="p">):</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;🎉 Excellent! You got all answers correct!&quot;</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">score</span> <span class="o">&gt;</span> <span class="nb">len</span><span class="p">(</span><span class="n">questions</span><span class="p">)</span> <span class="o">//</span> <span class="mi">2</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;😊 Good job! Keep practicing to perfect your understanding.&quot;</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;😕 Don&#39;t worry, review the concepts and try again!&quot;</span><span class="p">)</span>

<span class="n">score_button</span> <span class="o">=</span> <span class="n">Button</span><span class="p">(</span><span class="n">description</span><span class="o">=</span><span class="s2">&quot;Show Final Score&quot;</span><span class="p">,</span> <span class="n">button_style</span><span class="o">=</span><span class="s1">&#39;success&#39;</span><span class="p">)</span>
<span class="n">score_button</span><span class="o">.</span><span class="n">on_click</span><span class="p">(</span><span class="n">show_score</span><span class="p">)</span>
<span class="n">display</span><span class="p">(</span><span class="n">VBox</span><span class="p">([</span><span class="n">HTML</span><span class="p">(</span><span class="n">value</span><span class="o">=</span><span class="s2">&quot;&lt;hr&gt;&quot;</span><span class="p">),</span> <span class="n">score_button</span><span class="p">]))</span>

<span class="sd">&quot;&quot;&quot;## **References**</span>

<span class="sd">- Scikit-learn Documentation: https://scikit-learn.org/stable/</span>
<span class="sd">- SVM Kernels: https://scikit-learn.org/stable/modules/svm.html#</span>
<span class="sd">- Research Paper: Cortes, C., &amp; Vapnik, V. (1995). Support-vector networks. Machine learning, 20(3), 273-297.(https://link.springer.com/article/10.1007/BF00994018)</span>
<span class="sd">-Understanding Support Vector Machine(SVM) Algorithm from Examples:  https://www.geeksforgeeks.org/support-vector-machine-algorithm/(Analytics Vidhya)</span>

<span class="sd">- Wong, B. (2011). Color Universal Design (CUD): How to make figures and presentations that are friendly to colorblind people. https://jfly.uni-koeln.de/color/</span>

<span class="sd">- UCI Machine Learning Repository: Iris Dataset https://archive.ics.uci.edu/dataset/53/iris</span>
<span class="sd">- Gamma Parameters in SVM: https://www.geeksforgeeks.org/gamma-parameter-in-svm/</span>


<span class="sd">- Guidelines for creating visualizations that are accessible to colorblind individuals. https://jfly.uni-koeln.de/color/</span>


<span class="sd">- SVM Tutorial. &quot;Understanding the Math behind Support Vector Machines – Part 1.&quot; November 2014. [https://www.svm-tutorial.com/2014/11/svm-understanding-math-part-1/](https://www.svm-tutorial.com/2014/11/svm-understanding-math-part-1/).</span>


<span class="sd">&quot;&quot;&quot;</span>
</pre></div>
</body>
</html>
